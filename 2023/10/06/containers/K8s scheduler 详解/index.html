<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>Kubernetes scheduler详解 | 远辰</title><meta name="keywords" content="container"><meta name="author" content="哪吒藕霸"><meta name="copyright" content="哪吒藕霸"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="Kubernetes scheduler详解"><meta name="application-name" content="Kubernetes scheduler详解"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="Kubernetes scheduler详解"><meta property="og:url" content="https://shippomx.github.io/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/index.html"><meta property="og:site_name" content="远辰"><meta property="og:description" content="kube-scheduler 是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源。  kube-scheduler 是 kubernetes 系统的核心组件之一，主要"><meta property="og:locale" content="zh"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="哪吒藕霸"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="kube-scheduler 是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源。  kube-scheduler 是 kubernetes 系统的核心组件之一，主要"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="https://shippomx.github.io/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 哪吒藕霸","link":"链接: ","source":"来源: 远辰","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: '远辰',
  title: 'Kubernetes scheduler详解',
  postAI: '',
  pageFillDescription: '物理部署, 配置文件, 容器部署, 高可用, 预选策略, GeneralPredicates, Volume 相关, 宿主机相关, 其他, 优选策略, leastRequestedPriority, CalculateNodeLabelPriority, BalancedResourceAllocation, 其他, 亲和性, 指定 Node 节点调度, 亲和性（Affinity）与非亲和性（anti-affinity）, 屏蔽 Node 节点调度, 优先级调度, 抢占式调度, 调度器扩展程序, 多调度器, 调度框架, 简单实现一个调度扩展插件, 批调度, Binpack是系统的核心组件之一主要负责整个集群资源的调度功能根据特定的调度算法和策略将调度到最优的工作节点上面去从而更加合理更加充分的利用集群的资源是系统的核心组件之一主要负责整个集群资源的调度功能根据特定的调度算法和策略将调度到最优的工作节点上面去从而更加合理更加充分的利用集群的资源物理部署直接使用二进制文件启动就可以比如有很多具体使用的时候可以查看官网这边简单的说几个常用的配置文件策略文件配置文件根据上面参数配置配置文件我们来看看配置文件内容配置文件应该包含一个对象文件指定了调度器的一些参数包括选举调度算法策略的选择可以是也可以是具体的或者文件以及指定调度器的名称为这边我们还需要了解一下配置文件中指定的策略问题也可以直接使用启动参数进行配置这个是运行的关键可以看到配置预选和优选策略也是调度的算法默认有一个启动配置可以根据需要进行修改和扩展容器部署创建的配置文件和策略文件的创建一个名为的该配置文件应该包含一个对象下的文件指定了调度器的一些参数包括选举调度算法策略的选择指定另一个以及指定调度器的名称为相应的创建一个的里面指定了选择哪些预选优选策略以及外部扩展调度程序的扩展预选扩展优选扩展优先级抢占扩展扩展优选算法的权重等可以根据需求进行调整文件中将以文件的形式挂载到容器内目录下并在启动参数中指定启动高可用中的高可用是通过实现的一般三个哪一个先起来就是虽然两台机器上都安装了但是只有提供服务另外两个上面的是处于等待状态并没有真正运行自己的逻辑当异常后其他的服务就会成为继续提供服务当我们部署多个调度器的时候每个调度器都会各自调度属于自己的核心待调度的列表可有的合适的列表调度算法和策略首先客户端通过的或者工具创建资源收到用户请求后存储相关数据到数据库中将为空的放入调度器内部的调度队列中调度器监听查看为调度的列表循环遍历地为每个尝试分配节点从调度队列中出一个开始一个标准的调度周期预选阶段过滤节点调度器用一组规则过滤掉不符合要求的节点比如设置了资源的那么可用资源比需要的资源少的主机显然就会被过滤掉优选阶段为节点的优先级打分将上一阶段过滤出来的列表进行打分调度器会考虑一些整体的优化策略比如把控制的多个副本分布到不同的主机上使用最低负载的主机等等策略经过上面的阶段过滤后选择打分最高的节点和进行操作然后将结果存储到中最后被选择出来的节点对应的去执行创建的相关操作预选策略我们在部署应用的时候如果发现有一直处于状态那么就是没有满足调度条件的节点这个时候可以去检查下节点资源是否可用在中默认的调度策略有如下三种第一种类型叫作顾名思义这一组过滤规则负责的是最基础的调度策略比如计算的就是宿主机的和内存资源等是否够用资源要求判断备选节点资源是否满足备选的需求检测过程如下计算备选和节点中已存在的的所有容器的需求资源和内存的总和获得备选节点的状态信息其中包括节点的资源信息如果备选和节点中已存在的所有容器的需求资源和内存的总和超出了备选节点拥有的资源则返回表明备选节点不适合备选否则返回表明备选节点适合备选标签匹配判断备选节点是否包含备选的标签选择器指定的标签如果没有指定标签选择器则返回如果获得备选节点的标签信息判断节点是否包含备选的标签选择器所指的标签如果包含返回不包含返回判断备选的域所指定的节点名称和备选节点的名称是否一致如果一致返回否则返回判断备选所用的端口列表汇中的端口是否在备选节点中被占用如果被占用则返回否则返回节点上已经使用的是否和申请的冲突相关第二种类型是与相关的过滤规则这一组过滤规则负责的是跟容器持久化相关的调度策略磁盘冲突判断备选的或者和备选的节点中已存在的是否存在冲突具体检测过程如下首先读取备选的所有的信息对每一个执行一下步骤的冲突检测如果该是则将和备选节点上的所有的每个进行比较如果发现相同的则返回表明磁盘冲突检测结束反馈给调度器该备选节点不合适作为备选的如果是则将和备选节点上的所有的每个进行比较如果发现相同的则返回表明磁盘冲突检测结束反馈给调度器该备选节点不合适作为备选的最终检查备选的所有的均为发现冲突则返回表明不存在磁盘冲突反馈给调度器该备选节点合适备选检查的条件则是一个节点上某种类型的持久化是不是已经超过了一定数目如果是的话那么声明使用该类型持久化的就不能再调度到这个节点了则是检查持久化的高可用域标签是否与待考察节点的标签相匹配此外这里还有一个叫作的规则它负责检查的是该对应的的字段是否跟某个节点的标签相匹配宿主机相关第三种类型是宿主机相关的过滤规则这一组规则主要考察待调度是否满足本身的某些条件比如负责检查的就是我们前面经常用到的的污点机制只有当的字段与的字段能够匹配的时候这个才能被调度到该节点上其他过滤有一系列的算法可以使用上面就是简单的列举几个还有很多更多更详细的我们可以查看源码文件虽然是串行的但是当开始调度一个时调度器会同时启动个来并发地为集群里的所有计算最后返回可以运行这个的宿主机列表优选策略在阶段完成了节点的过滤之后阶段的工作就是为这些节点打分这里打分的范围是分得分最高的节点就是最后被绑定的最佳节点里最常用到的一个打分规则是该策略用于从备选节点列表中选出资源消耗最小的节点计算出所有备选节点上运行的和备选的占用量计算出所有备选节点上运行的和备选的占用量根据特定的算法计算每个节点的得分公式如下实际上就是在选择空闲资源和最多的宿主机如果用户在配置中指定了该策略则会通过方法注册该策略该策略用于判断策略列出的标签在备选节点中存在时是否选择该备选节点如果备选节点的标签在优选策略的标签列表中且优选策略的值为或者备选节点的标签不在优选策略的标签列表中且优选策略的值为则备选节点否则等于该优选策略用于从备选节点列表中选出各项资源使用率最均衡的节点计算出所有备选节点上运行的和备选的占用量计算出所有备选节点上运行的和备选的占用量根据特定的算法计算每个节点的得分公式请求的资源节点上的可用资源而算法的作用则是计算每两种资源之间的距离而最后选择的则是资源差距最小的节点所以说选择的其实是调度完成后所有节点里各种资源分配最均衡的那个节点从而避免一个节点上被大量分配而大量剩余的情况其他还有很多其他的策略比如如下等等为了更好的高可用对同属于一个或者下面的多个副本尽量调度到多个不同的节点上当一个被调度的时候会先去查找该对应的然后查看该下面的已存在的运行越少的节点权重越高就是如果在某个节点上已经有要使用的镜像节点了镜像总大小值越大权重就越高这个就是根据节点的亲和性来计算一个权重值后面我们会详细讲解亲和性的使用方法同样我们可以查看源码文件了解更多信息每一个优先级函数会返回一个的分数分数越高表示节点越优同时每一个函数也会对应一个表示权重的值最终主机的得分用以下公式计算得出集合中的每一个函数都有一个权重最终的值为和的乘积而一个节点的就是所有结果的加和亲和性在我们实际使用中有着很多的使用的地方比如指定屏蔽指定节点调度有三种方式指定只运行在指定的节点上只调度到匹配指定的上功能更丰富的选择器比如支持集合操作调度到满足条件的所在的上首先给打上标签然后在中指定为根据默认调度器的策略选出合适的节点然后打分进行分配目前支持两种和分别代表必须满足条件和优选条件比如下面的例子代表调度到包含标签并且值为或的上并且优选还带有标签的基于的标签来选择仅调度到满足条件所在的上支持和这个功能比较绕以下面的例子为例如果一个所在中包含至少一个带有标签且运行中的那么可以调度到该不调度到包含至少一个带有标签且运行中的上可以看出基本上都是根据进行调度选择亲和性与非亲和性在上面会说亲和性的操作更加多样化所以我们一般都是使用亲和性而不是使用在调度中亲和性主要分为种类型与我们在的调度的时候说明过亲和性调度的使用主要是两个策略强制和优先调度到符合条件的上去用于调度可以和哪些部署在同一拓扑结构之下相反其用于规定不可以和哪些部署在同一拓扑结构下同样有两个策略强制和优先屏蔽节点调度和用于保证不被调度到不合适的上其中应用于上而则应用于上目前支持的类型新的不调度到该上不影响正在运行的版的尽量不调度到该上新的不调度到该上并且删除已在运行的可以增加一个时间然而当的匹配的所有的时候可以调度到该上当是已经运行的时候也不会被删除另外对于如果增加了一个则会在该时间之后才删除比如假设上应用以下几个下面的这个由于没有无法调度到上而正在运行且带有的则会在之后删除注意创建的会自动加上对和的以避免它们因此被删除可以看到的调度器的核心实际上就是两个相互独立的控制循环第一个控制循环我们可以称之为它的主要目的是启动一系列用来监听中等与调度相关的对象的变化比如当一个待调度即它的字段是空的被创建出来之后调度器就会通过的将这个待调度添加进调度队列在默认情况下的调度队列是一个优先级队列并且当某些集群信息发生变化的时候调度器还会对调度队列里的内容进行一些特殊操作这里的设计主要是出于调度优先级和抢占的考虑我会在后面再详细介绍这部分内容此外的默认调度器还要负责对调度器缓存即进行更新事实上调度部分进行性能优化的一个最根本原则就是尽最大可能将集群信息化以便从根本上提高和调度算法的执行效率第二个控制循环是调度器负责调度的主循环我们可以称之为的主要逻辑就是不断地从调度队列里出队一个然后调用算法进行过滤这一步过滤得到的一组就是所有可以运行这个的宿主机列表当然算法需要的信息都是从里直接拿到的这是调度器保证算法执行效率的主要手段之一接下来调度器就会再调用算法为上述列表里的打分分数从到得分最高的就会作为这次调度的结果其实这就是我们上面的说的调度过程也是我们核心关注的地方后面的调度框架也是这部分的扩展调度算法执行完成后调度器就需要将对象的字段的值修改为上述的名字这个步骤在里面被称作但是为了不在关键调度路径里远程访问的默认调度器在阶段只会更新里的和的信息这种基于乐观假设的对象更新方式在里被称作之后调度器才会创建一个来异步地向发起更新的请求来真正完成操作如果这次异步的过程失败了其实也没有太大关系等同步之后一切就会恢复正常当然正是由于上述调度器的乐观绑定的设计当一个新的完成调度需要在某个节点上运行起来之前该节点上的还会通过一个叫作的操作来再次验证该是否确实能够运行在该节点上这一步操作实际上就是把一组叫作的最基本的调度算法比如资源是否可用端口是否冲突等再执行一遍作为端的二次确认除了上述的化和乐观绑定默认调度器还有一个重要的设计那就是无锁化在上调度器会启动多个以节点为粒度并发执行算法从而提高这一阶段的执行效率而与之类似的算法也会以的方式并行计算然后再进行汇总而在这些所有需要并发的路径上调度器会避免设置任何全局的竞争资源从而免去了使用锁进行同步带来的巨大的性能损耗所以在这种思想的指导下如果你再去查看一下前面的调度器原理图你就会发现调度器只有对调度队列和进行操作时才需要加锁而这两部分操作都不在的算法执行路径上调度器的上述设计思想也是在集群规模不断增长的演进过程中逐步实现的尤其是化这个变化其实是最近几年调度器性能得以提升的一个关键演化优先级调度规定优先级是一个的整数最大值不超过亿并且值越大代表优先级越高而超出亿的值其实是被保留下来分配给系统使用的而对于没有声明的来说它们的优先级就是的优先级高优先级的会优先被调度或者在资源不足低情况牺牲低优先级的以便于重要的能够得到资源部署要定义优先级就需要先定义对象该对象没有的限制其中为位整数的优先级该值越大优先级越高用于未配置的整个集群中应该只有一个将其设置为然后通过在的中指定已定义的名称即可另外一个值得注意的是当节点没有足够的资源供调度器调度导致处于时抢占逻辑就会被触发会尝试从一个节点删除低优先级的从而释放资源使高优先级的得到节点资源进行部署抢占式调度而当一个高优先级的调度失败的时候调度器的抢占能力就会被触发这时调度器就会试图从当前集群里寻找一个节点使得当这个节点上的一个或者多个低优先级被删除后待调度的高优先级就可以被调度到这个节点上当上述抢占过程发生时抢占者并不会立刻被调度到被抢占的上事实上调度器只会将抢占者的字段设置为被抢占的的名字然后抢占者会重新进入下一个调度周期然后在新的调度周期里来决定是不是要运行在被抢占的节点上这当然也就意味着即使在下一个调度周期调度器也不会保证抢占者一定会运行在被抢占的节点上这样设计的一个重要原因是调度器只会通过标准的来删除被抢占的所以这些必然是有一定的优雅退出时间默认是的而在这段时间里其他的节点也是有可能变成可调度的或者直接有新的节点被添加到这个集群中来所以鉴于优雅退出期间集群的可调度性可能会发生的变化把抢占者交给下一个调度周期再处理是一个非常合理的选择而在抢占者等待被调度的过程中如果有其他更高优先级的也要抢占同一个节点那么调度器就会清空原抢占者的字段从而允许更高优先级的抢占者执行抢占并且这也就使得原抢占者本身也有机会去重新抢占其他节点这些都是设置字段的主要目的调度器里的抢占机制原理调度器实现抢占算法的一个最重要的设计就是在调度队列的实现里使用了两个不同的队列第一个队列叫作凡是在里的都是下一个调度周期需要调度的对象所以当你在集群里新创建一个的时候调度器会将这个入队到里面而我在前面提到过的调度器不断从队列里出队一个进行调度实际上都是从里出队的第二个队列叫作专门用来存放调度失败的调度失败之后抢占者就会被放进里面然后这次失败事件就会触发调度器为抢占者寻找牺牲者的流程第一步调度器会检查这次失败事件的原因来确认抢占是不是可以帮助抢占者找到一个新节点这是因为有很多的失败是不能通过抢占来解决的比如算法负责的是检查的与的名字是否匹配这种情况下除非的名字发生变化否则你即使删除再多的抢占者也不可能调度成功当遍历完所有的节点之后调度器会在上述模拟产生的所有抢占结果里做一个选择找出最佳结果而这一步的判断原则就是尽量减少抢占对整个系统的影响比如需要抢占的越少越好需要抢占的的优先级越低越好等等第二步如果确定抢占可以发生那么调度器就会把自己缓存的所有节点信息复制一份然后使用这个副本来模拟抢占过程在得到了最佳的抢占结果之后这个结果里的就是即将被抢占的被删除的列表就是牺牲者所以接下来调度器就可以真正开始抢占的操作了这个过程可以分为三步第一步调度器会检查牺牲者列表清理这些所携带的字段第二步调度器会把抢占者的设置为被抢占的的名字第三步调度器会开启一个同步地删除牺牲者这里的抢占过程很容易理解调度器会检查缓存副本里的每一个节点然后从该节点上最低优先级的开始逐一删除这些而每删除一个低优先级调度器都会检查一下抢占者是否能够运行在该上一旦可以运行调度器就记录下这个的名字和被删除的列表这就是一次抢占过程的结果了调度器就会对这个将同样的算法运行两遍第一遍调度器会假设上述潜在的抢占者已经运行在这个节点上然后执行算法第二遍调度器会正常执行算法即不考虑任何潜在的抢占者不难想到这里需要执行第一遍算法的原因是由于规则的存在由于规则关心待考察节点上所有之间的互斥关系所以我们在执行调度算法时必须考虑如果抢占者已经存在于待考察上时待调度还能不能调度成功当然这也就意味着我们在这一步只需要考虑那些优先级等于或者大于待调度的抢占者毕竟对于其他较低优先级来说待调度总是可以通过抢占运行在待考察上而我们需要执行第二遍算法的原因则是因为潜在的抢占者最后不一定会运行在待考察的上关于这一点我在前面已经讲解过了调度器并不保证抢占者一定会运行在当初选定的被抢占的上要编写一个优秀的调度器却不容易因为要考虑的东西很多尽可能地将平均到不同的节点减少单个节点宕机造成的损失可扩展性随着集群规模的增加怎么保证调度器不会成为性能的瓶颈高可用调度器能做组成集群任何一个调度器出现问题不会影响整个集群的调度灵活性不同的用户有不同的调度需求一个优秀的调度器还要允许用户能配置不同的调度算法资源合理和高效利用调度器应该尽可能地提高集群的资源利用率防止资源的浪费一般来说我们有种扩展调度器的方法直接官方的源代码在合适的位置直接修改代码然后重新编译运行修改后的程序当然这种方法是最不建议使用的也不实用因为需要花费大量额外的精力来和上游的调度程序更改保持一致和默认的调度程序一起运行独立的调度程序默认的调度器和我们自定义的调度器可以通过的来覆盖各自的默认是使用默认的调度器但是多个调度程序共存的情况下也比较麻烦比如当多个调度器将调度到同一个节点的时候可能会遇到一些问题因为很有可能两个调度器都同时将两个调度到同一个节点上去但是很有可能其中一个运行后其实资源就消耗完了并且维护一个高质量的自定义调度程序也不是很容易的因为我们需要全面了解默认的调度程序整体的架构知识以及各种对象的各种关系或限制调度器扩展程序这个方案目前是一个可行的方案可以和上游调度程序兼容所谓的调度器扩展程序其实就是一个可配置的而已里面包含过滤器和优先级两个端点分别对应调度周期中的两个主要阶段过滤和打分通过调度框架版本中引入了可插拔架构的调度框架使得定制调度器这个任务变得更加的容易调库框架向现有的调度器中添加了一组插件化的该在保持调度程序核心简单且易于维护的同时使得大部分的调度功能以插件的形式存在而且在我们现在的版本中上面的调度器扩展程序也已经被废弃了所以以后调度框架才是自定义调度器的核心方式这里我们可以简单介绍下后面三种方式的实现调度器扩展程序实现扩展程序我们直接用来实现一个简单的调度器扩展程序当然你可以使用其他任何编程语言如下所示然后接下来我们需要实现和两个端点的处理程序其中这个扩展函数接收一个输入类型为的参数然后返回一个类型为的值在函数中我们可以进一步过滤输入的节点根据扩展程序定义的预选规则来过滤节点在过滤函数中我们循环每个节点然后用我们自己实现的业务逻辑来判断是否应该批准该节点这里我们实现比较简单在函数中我们只是简单的检查随机数是否为偶数来判断即可如果是的话我们就认为这是一个幸运的节点否则拒绝批准该节点同样的打分功能用同样的方式来实现我们在每个节点上随机给出一个分数这个函数输出的分数会被添加会默认的调度器在最大优先级内随机取一个值然后我们可以使用下面的命令来编译打包我们的应用获得我们的扩展调度器扩展程序的二进制文件构建完成后将应用拷贝到所在的节点直接运行即可当然也可以使用运行复制一个调度器的文件然后更改下来部署这样就不会影响默认的调度器了然后在需要使用这个测试的调度器的上面指定即可这就是第二种方法的多调度器注册扩展程序我们只要在策略的配置文件中注册就可以格式是将其写到默认的文件中然后启动就行这时候我们在调度的是时候是重上到下这样在过滤和打分阶段结束后可以将结果分别传递给该扩展程序的端点可以进一步过滤并确定优先级以适应我们的特定业务需求同一个调度器就是这样多个调度器时候每一个调度器都是这么个重下倒下过滤调度的流程验证现在我们来运行一个查看其工作原理我们准备一个包含个副本的部署直接创建上面的资源对象这个时候我们去查看下我们编写的调度器扩展程序日志我们可以看到调度的过程另外默认调度程序会定期重试失败的因此它们将一次又一次地重新传递到我们的调度扩展程序上我们的逻辑是检查随机数是否为偶数所以最终所有都将处于运行状态调度器扩展程序可能是在一些情况下可以满足我们的需求但是他仍然有一些限制和缺点通信成本数据在默认调度程序和调度器扩展程序之间以传输在执行序列化和反序列化的时候有一定成本有限的扩展点扩展程序只能在某些阶段的末尾参与例如和它们不能在任何阶段的开始或中间被调用减法优于加法与默认调度程序传递的节点候选列表相比我们可能有一些需求需要添加新的候选节点列表但这是比较冒险的操作因为不能保证新节点可以通过其他要求所以调度器扩展程序最好执行减法进一步过滤而不是加法添加节点缓存共享上面只是一个简单的测试示例但在真实的项目中我们是需要通过查看整个集群的状态来做出调度决策的默认调度程序可以很好地调度决策但是无法共享其缓存这意味着我们必须构建和维护自己的缓存由于这些局限性调度小组就提出了上面第四种方法来进行更好的扩展也就是调度框架它基本上可以解决我们遇到的所有难题现在也已经成官方推荐的扩展方式所以这将是以后扩展调度器的最主流的方式多调度器上面我们已经开发了一个调度器我们只要将对应的二进制文件制作成镜像然后运行在上并将这个调度器命一个名比如给后来的进行指定当然可以参考官方文档在整个集群中还可以同时运行多个调度器实例通过来选择使用哪一个调度器默认使用内置的调度器选择使用自定义调度器调度框架将的调度过程分为两步调度过滤打分和绑定延时绑定和绑定调度是为选择一个合适的节点而绑定则是将调度结果提交给集群调度是顺序执行的绑定并发执行无论是在调度还是绑定过程中如果发生错误或者判断不可调度那么就会被重新放回调度队列等待重新调度调度框架定义了一组扩展点用户可以实现扩展点定义的接口来定义自己的调度逻辑并将扩展注册到扩展点上调度框架在执行调度工作流时遇到对应的扩展点时将调用用户注册的扩展调度框架在预留扩展点时都是有特定的目的有些扩展点上的扩展可以改变调度程序的决策方法有些扩展点上的扩展只是发送一个通知下图展示了调度框架中的调度上下文及其中的扩展点一个扩展可以注册多个扩展点以便可以执行更复杂的有状态的任务做一个简单的说明扩展用于对的待调度队列进行排序以决定先调度哪个扩展本质上只需要实现一个方法用于比较两个谁更优先获得调度即可同一时间点只能有一个插件生效扩展用于对的信息进行预处理或者检查一些集群或必须满足的前提条件如果返回了则调度过程终止扩展用于排除那些不能运行该的节点对于每一个节点调度器将按顺序执行扩展如果任何一个将节点标记为不可选则余下的扩展将不会被执行调度器可以同时对多个节点执行扩展是一个通知类型的扩展点调用该扩展的参数是阶段结束后被筛选为可选节点的节点列表可以在扩展中使用这些信息更新内部状态或者产生日志或信息扩展用于为所有可选节点进行打分调度器将针对每一个节点调用扩展评分结果是一个范围内的整数在阶段调度器将会把每个扩展对具体某个节点的评分结果和该扩展的权重合并起来作为最终评分结果扩展在调度器对节点进行最终排序之前修改每个节点的评分结果注册到该扩展点的扩展在被调用时将获得同一个插件中的扩展的评分结果作为参数调度框架每执行一次调度都将调用所有插件中的一个扩展一次是一个通知性质的扩展点有状态的插件可以使用该扩展点来获得节点上为预留的资源该事件发生在调度器将绑定到节点之前目的是避免调度器在等待与节点绑定的过程中调度新的到节点上时发生实际使用资源超出可用资源的情况因为绑定到节点上是异步发生的这是调度过程的最后一个步骤进入状态以后要么在绑定失败时触发扩展要么在绑定成功时由扩展结束绑定过程扩展用于阻止或者延迟与节点的绑定扩展可以做下面三件事中的一项批准当所有的扩展都了与节点的绑定调度器将继续执行绑定过程拒绝如果任何一个扩展了与节点的绑定将被放回到待调度队列此时将触发扩展等待如果一个扩展返回了则将保持在阶段直到被其他扩展如果超时事件发生状态变成将被放回到待调度队列此时将触发扩展扩展用于在绑定之前执行某些逻辑例如扩展可以将一个基于网络的数据卷挂载到节点上以便可以使用如果任何一个扩展返回错误将被放回到待调度队列此时将触发扩展扩展用于将绑定到节点上只有所有的扩展都成功执行了扩展才会执行调度框架按照扩展注册的顺序逐个调用扩展具体某个扩展可以选择处理或者不处理该如果某个扩展处理了该与节点的绑定余下的扩展将被忽略是一个通知性质的扩展扩展在成功绑定到节点上之后被动调用扩展是绑定过程的最后一个步骤可以用来执行资源清理的动作是一个通知性质的扩展如果为预留了资源又在被绑定过程中被拒绝绑定则扩展将被调用扩展应该释放已经为预留的节点上的计算资源在一个插件中扩展和扩展应该成对出现一个可以实现多个扩展点即在一个中既可以实现又可以实现也可以再实现看具体需求和场景避免了一个需求实现多个的情况上述这些可插拔式逻辑都是标准的语言插件机制机制也就是说你需要在编译的时候选择把哪些插件编译进去如果我们要实现自己的插件必须向调度框架注册插件并完成配置另外还必须实现扩展点接口对应的扩展点接口我们可以在源码文件中找到如下所示我们要实现对应的插件只要实现上面对应的插件接口也可以在一个进程中实现那多个插件我们可以看到每个插件都组合了的结构体这个就是默认的实现方式也就是我们对应的原生的策略最终使用什么还是要在策略文件中使用来完成指定插件名的调用对于调度框架插件的启用或者禁用我们同样可以使用上面的资源对象来进行配置下面的例子中的配置启用了一个实现了和扩展点的插件并且禁用了另外一个插件同时为插件提供了一些配置信息插件可以解析的任意内容扩展的调用顺序如下如果某个扩展点没有配置对应的扩展调度框架将使用默认插件中的扩展如果为某个扩展点配置且激活了扩展则调度框架将先调用默认插件的扩展再调用配置中的扩展默认插件的扩展始终被最先调用然后按照中扩展的激活顺序逐个调用扩展点的扩展可以先禁用默认插件的扩展然后在列表中的某个位置激活默认插件的扩展这种做法可以改变默认插件的扩展被调用时的顺序假设默认插件实现了扩展点此时我们要添加一个插件想要在之前被调用则应该先禁用再按照的顺序激活示例配置如下所示在源码目录中有几个示范插件我们可以参照其实现方式简单实现一个调度扩展插件注册其实要实现一个调度框架的插件并不难我们只要实现对应的扩展点然后将插件注册到调度器中即可我们来看一下注册其中就是注册一个名为的插件接下来就是我们接下来要实现的插件从函数的参数也可以看出我们这里的必须是一个类型的值而的定义就是一个函数我们简单看一下的函数创建的就是的列表然后给调用也就是函数中的调用的实现所以实际上就是上面的这个函数在这个函数中我们可以获取到插件中的一些数据然后进行逻辑处理即可插件实现如下所示我们这里只是简单获取下数据打印日志如果你有实际需求的可以根据获取的数据就行处理即可我们这里只是实现了三个扩展点其他的可以用同样的方式来扩展即可插件名称完整代码可以前往仓库获取打包运行实现完成后编译打包成镜像即可然后我们就可以当成普通的应用用一个控制器来部署即可由于我们需要去获取集群中的一些资源对象所以当然需要申请权限然后同样通过参数来配置我们的调度器同样还是使用一个资源对象配置可以通过来启用或者禁用我们实现的插件也可以通过来传递一些参数值给插件直接部署上面的资源对象即可这样我们就部署了一个名为的调度器了测试接下来我们可以部署一个应用来使用这个调度器进行调度这里需要注意的是我们现在手动指定了一个的字段将其设置成上面我们自定义的调度器名称我们直接创建这个资源对象创建完成后查看我们自定义调度器的日志信息可以看到当我们创建完后在我们自定义的调度器中就出现了对应的日志并且在我们定义的扩展点上面都出现了对应的日志证明我们的示例成功了也可以通过查看的来验证在最新的版本中内置的预选和优选函数已经全部插件化所以要扩展调度器我们应该掌握并理解调度框架这种方式所以以后调度框架是必然的趋势我们所要做的工作就是将我们开发的调度器或扩展程序移植到我们对应的调度框架中去调度器的源码位于中大体的代码目录结构如下所示不同的版本目录结构可能不太一样调度相关的具体实现节点筛选策略节点打分策略定义默认的调度器其中创建和运行的核心程序对应的代码在如果要查看的入口程序对应的代码在主要调度就是上面的流程批调度的定义是在并发系统中将多个相关联的进程调度到不同处理器上同时运行的策略在的场景中最主要的原则是保证所有相关联的进程能够同时启动防止部分进程的异常导致整个关联进程组的阻塞这种导致阻塞的部分异常进程称之为碎片在的具体实现过程中根据是否允许碎片存在可以细分为和其中就是大家常听到的要求完全不允许有碎片存在也就是简单来说一个批任务关联进程组包括了个进程调度器负责将这个调度到个节点处理器上同时运行如果这个批任务需要部分同时启动即可运行我们称需启动的最小数量为特别地当时批任务要求满足为什么需要批调度需要个同时启动才能正常运行依次调度个并创建成功到第个时集群资源不足则的个处于空等的状态但是它们已经占用了部分资源如果第个不能及时启动的话整个无法成功运行更糟糕的是导致集群资源浪费如果出现更坏的情况的话如下图所示集群其他的资源刚好被的个所占用同时在等待的第个创建此时整个集群就出现了死锁解决社区目前有以及基于衍生的个项目来解决上文中提到的痛点实现的方式是通过开发新的调度器将中的调度单元从修改为以组的形式进行调度使用方式是如果需要功能的走新的调度器这些方案虽然能够解决的问题但是同样引入了新的问题如大家所知对于同一集群资源调度器需要中心化但如果同时存在两个调度器的话有可能会出现决策冲突例如分别将同一块资源分配给两个不同的导致某个调度到节点后因为资源不足导致无法创建的问题解决的方式只能是通过标签的形式将节点强行的划分开来或者部署多个集群这种方式通过同一个集群来同时运行在线服务和离线作业势必会导致整体集群资源的浪费以及运维成本的增加再者运行需要启动定制的和这些本身存在单点风险一旦出现故障将影响集群内所有的创建另外多运行一套调度器本身也会带来维护上的复杂性以及与上游接口兼容上的不确定性基于的方案实现我们通过的形式来定义的概念拥有同样的同属于一个是用来标识该的作业能够正式运行时所需要的最小副本数备注要求属于同一个的必须保持相同的优先级的插件提供了延迟绑定的功能即进入到阶段时用户可以自定义条件来允许通过拒绝通过以及让等待状态可设置超时时间的延迟绑定的功能刚好可以让属于同一个的调度到这个节点时进行等待等待积累的数目满足足够的数目时再统一运行同一个的所有进行绑定并创建举个实际的例子当调度时需要个同时启动才能正常运行但此时集群仅能满足个创建此时与不同的是并不是直接将个调度并创建而是通过的机制进行等待此时当集群中有空闲资源被释放后的中所需要的资源均可以满足则的个被一起调度创建出来正常运行任务由于的队列并不能感知的信息所以在出队时处于无序性针对而言如下图所示和表示两个不同的两个的在进入队列时由于创建的时间交错导致在队列中以交错的顺序排列当一个新的创建后入队后无法跟与其相同的的排列在一起只能继续以混乱的形式交错排列这种无序性就会导致如果在阶段处于等待状态此时的调度完成后也处于等待状态相互占有资源使得和均无法正常调度这种情况即是把死锁现象出现的位置从节点移动到阶段无法解决前文提到的问题针对如上所示的问题我们通过实现插件保证在队列中属于同一个的能够排列在一起我们通过定义所用的方法作用于在入队后排队的顺序首先继承了默认的基于优先级的比较方式高优先级的会排在低优先级的之前然后如果两个的优先级相同我们定义了新的排队逻辑来支持的排序如果两个都是普通的则谁先创建谁在队列里边排在前边如果两个一个是另一个是属于某个的我们比较的是的创建时间和所属的创建时间则谁先创建谁在队列里边排在前边如果两个都是我们比较两个的创建时间则谁先创建谁在队列里边排在前边同时有可能两个的创建时间相同我们引入了自增使得的谁小谁排在前边此处的目的是为了区分不同的通过如上的排队策略我们实现属于同一个的能够同一个的能够排列在一起当一个新的创建后入队后会跟与其相同的的排列在一起为了减少无效的调度操作提升调度的性能我们在阶段增加一个过滤条件当一个调度时会计算该所属的的包括状态的如果小于时则肯定无法满足的要求则直接在阶段拒绝掉不再进入调度的主流程如果某个在阶段等待超时了则会进入到阶段我们会直接拒绝掉所有跟属于同一个的避免剩余的进行长时间的无效等待为什么需要功能默认开启的资源调度策略是消耗的资源最少的节点会优先被调度使得整体集群的资源使用在所有节点之间分配地相对均匀但是这种调度策略往往也会在单个节点上产生较多资源碎片比如两个节点各剩余的资源这是有申请的新作业提交到调度器则因为无法提供足够的资源导致调度失败如上这种情况情况每个节点都有个卡空闲可是又无法被利用导致资源这种昂贵的资源被浪费如果使用的资源调度策略是优先将节点资源填满之后再调度下一个节点则上图所出现的资源碎片问题得到解决申请的作业被正常调度到节点上提升了集群的资源使用率实现已经抽象成的插件根据已分配资源的某函数设置选择节点实现的扩展点用于优选阶段给节点打分将节点根据自己定义的配置进行打分具体的实现可以分为两个部分构建打分函数和打分构建打分函数的过程比较容易理解就是用户可以自己定义不同的利用率所对应的分值大小以便影响调度的决策过程即如果资源利用率为的时候得分为分当资源利用率为时得分为分所以得到的资源利用率越高得分越高则这个行为是的资源分配方式用户也可以设置成利用率为时得分为分利用率为时得分为分这样意味着资源利用率越低则得分越高这种行为是的资源分配方式打分用户可以自己定义在计算中所要参考的资源以及权重值例如可以只是设定和的值和权重然后在打分过程总会通过计算的结果得到对应资源的利用率并且将利用率带入上文中所述的打分函数中得到相应的分数最后将所有的资源根据值加权得到最终的分数',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-08 11:29:14',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">远辰</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/algorithm/" style="font-size: 1.05rem;">algorithm<sup>1</sup></a><a href="/tags/blockchain/" style="font-size: 1.05rem;">blockchain<sup>1</sup></a><a href="/tags/c/" style="font-size: 1.05rem;">c<sup>1</sup></a><a href="/tags/container/" style="font-size: 1.05rem;">container<sup>27</sup></a><a href="/tags/go/" style="font-size: 1.05rem;">go<sup>10</sup></a><a href="/tags/kidgets/" style="font-size: 1.05rem;">kidgets<sup>3</sup></a><a href="/tags/linux/" style="font-size: 1.05rem;">linux<sup>22</sup></a><a href="/tags/rust/" style="font-size: 1.05rem;">rust<sup>6</sup></a><a href="/tags/tools/" style="font-size: 1.05rem;">tools<sup>1</sup></a><a href="/tags/tvbox/" style="font-size: 1.05rem;">tvbox<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">11</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/07/"><span class="card-archive-list-date">July 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">8</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/06/"><span class="card-archive-list-date">June 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">9</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/05/"><span class="card-archive-list-date">May 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/04/"><span class="card-archive-list-date">April 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/03/"><span class="card-archive-list-date">March 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/02/"><span class="card-archive-list-date">February 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/12/"><span class="card-archive-list-date">December 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"><a class="article-meta__tags" href="/tags/container/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>container</span></a></span></div></div><h1 class="post-title" itemprop="name headline">Kubernetes scheduler详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2023-10-06T06:44:56.000Z" title="发表于 2023-10-06 14:44:56">2023-10-06</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2023-10-08T03:29:14.458Z" title="更新于 2023-10-08 11:29:14">2023-10-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://shippomx.github.io/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/"><header><a href="/tags/container/" tabindex="-1" itemprop="url">container</a><h1 id="CrawlerTitle" itemprop="name headline">Kubernetes scheduler详解</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">哪吒藕霸</span><time itemprop="dateCreated datePublished" datetime="2023-10-06T06:44:56.000Z" title="发表于 2023-10-06 14:44:56">2023-10-06</time><time itemprop="dateCreated datePublished" datetime="2023-10-08T03:29:14.458Z" title="更新于 2023-10-08 11:29:14">2023-10-08</time></header><blockquote>
<p>kube-scheduler 是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源。</p>
</blockquote>
<p>kube-scheduler 是 kubernetes 系统的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源。</p>
<h2 id="物理部署"><a href="#物理部署" class="headerlink" title="物理部署"></a>物理部署</h2><p>直接使用二进制文件启动就可以</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-scheduler [flags]</span><br></pre></td></tr></table></figure>

<p>比如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/kube-scheduler --logtostderr=true --v=4 --master=http://10.243.129.252:8080 --address=0.0.0.0 --master=http://10.243.129.252:8080 --leader-elect=true --v=5 --log-dir=/k8s_log/kubernetes --use-legacy-policy-config=true --policy-config-file=/etc/kubernetes/scheduler-policy.config</span><br></pre></td></tr></table></figure>

<p>flags 有很多，具体使用的时候可以查看<a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-scheduler/">官网</a>。</p>
<p>这边简单的说几个常用的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//配置文件</span><br><span class="line">--config=/etc/kubernetes/config/kube-scheduler.yaml</span><br><span class="line">//策略文件</span><br><span class="line">--use-legacy-policy-config=true</span><br><span class="line">--policy-config-file=/etc/kubernetes/scheduler-policy.config</span><br></pre></td></tr></table></figure>

<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>根据上面参数配置配置文件，我们来看看配置文件内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/kubernetes/config/kube-scheduler.yaml</span><br><span class="line">apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line">schedulerName: my-kube-scheduler</span><br><span class="line">algorithmSource:</span><br><span class="line">  policy:</span><br><span class="line">    configMap:</span><br><span class="line">      namespace: kube-system</span><br><span class="line">      name: my-scheduler-policy</span><br><span class="line">leaderElection:</span><br><span class="line">  leaderElect: false</span><br><span class="line">  lockObjectName: my-kube-scheduler</span><br><span class="line">  lockObjectNamespace: kube-system</span><br></pre></td></tr></table></figure>

<p>配置文件应该包含一个 KubeSchedulerConfiguration 对象，yaml 文件指定了调度器的一些参数，包括 leader 选举，调度算法策略的选择（可以是 configmaps，也可以是具体的 json 或者 yaml 文件），以及指定调度器的名称为 my-kube-scheduler。</p>
<p>这边我们还需要了解一下配置文件中指定的策略问题，也可以直接使用启动参数进行配置–use-legacy-policy-config&#x3D;true –policy-config-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;scheduler-policy.config，这个是 scheduler 运行的关键。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/kubernetes/scheduler-policy.config</span><br><span class="line">&#123;</span><br><span class="line">&quot;kind&quot; : &quot;Policy&quot;,</span><br><span class="line">&quot;apiVersion&quot; : &quot;v1&quot;,</span><br><span class="line">&quot;predicates&quot; : [</span><br><span class="line">    &#123;&quot;name&quot; : &quot;CheckNodeUnschedulable&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;GeneralPredicates&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;NoDiskConflict&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;PodToleratesNodeTaints&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;LimitSRIOVQuantity&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;Reschedule&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;Mutex&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;AppLimit&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;HAschedule&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;CheckVolumeBinding&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;MaxOssBucketCount&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;MaxCSIVolumeCountPred&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;MatchInterPodAffinity&quot;&#125;</span><br><span class="line">    ],</span><br><span class="line">&quot;priorities&quot; : [</span><br><span class="line">    &#123;&quot;name&quot; : &quot;AppLimitPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;AppServiceSpreadPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;BalancedResourceAllocation&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;DiskIOPSPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;GpuBinpackPriority&quot;, &quot;weight&quot; : 20&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;HAschedulerPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;ImageLocalityPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;InterPodAffinityPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;NetworkBandwidthPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;NodeAffinityPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;ReschedulePriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;SelectorSpreadPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;LeastRequestedPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">    &#123;&quot;name&quot; : &quot;TaintTolerationPriority&quot;, &quot;weight&quot; : 1&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到配置预选和优选策略，也是调度的算法，默认有一个启动配置，可以根据需要进行修改和扩展。</p>
<h2 id="容器部署"><a href="#容器部署" class="headerlink" title="容器部署"></a>容器部署</h2><p>1、创建 kube-scheduler 的配置文件和策略文件的 configmap</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: my-scheduler-config</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  config.yaml: |</span><br><span class="line">    apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">    kind: KubeSchedulerConfiguration</span><br><span class="line">    schedulerName: my-kube-scheduler</span><br><span class="line">    algorithmSource:</span><br><span class="line">      policy:</span><br><span class="line">        configMap:</span><br><span class="line">          namespace: kube-system</span><br><span class="line">          name: my-scheduler-policy</span><br><span class="line">    leaderElection:</span><br><span class="line">      leaderElect: false</span><br><span class="line">      lockObjectName: my-kube-scheduler</span><br><span class="line">      lockObjectNamespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: my-scheduler-policy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line"> policy.cfg : |</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;kind&quot; : &quot;Policy&quot;,</span><br><span class="line">    &quot;apiVersion&quot; : &quot;v1&quot;,</span><br><span class="line">    &quot;predicates&quot; : [</span><br><span class="line">      &#123;&quot;name&quot; : &quot;PodFitsHostPorts&quot;&#125;,</span><br><span class="line">      &#123;&quot;name&quot; : &quot;PodFitsResources&quot;&#125;,</span><br><span class="line">      &#123;&quot;name&quot; : &quot;NoDiskConflict&quot;&#125;,</span><br><span class="line">      &#123;&quot;name&quot; : &quot;MatchNodeSelector&quot;&#125;,</span><br><span class="line">      &#123;&quot;name&quot; : &quot;HostName&quot;&#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;priorities&quot; : [</span><br><span class="line">      &#123;&quot;name&quot; : &quot;LeastRequestedPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">      &#123;&quot;name&quot; : &quot;BalancedResourceAllocation&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">      &#123;&quot;name&quot; : &quot;ServiceSpreadingPriority&quot;, &quot;weight&quot; : 1&#125;,</span><br><span class="line">      &#123;&quot;name&quot; : &quot;EqualPriority&quot;, &quot;weight&quot; : 1&#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;extenders&quot; : [&#123;</span><br><span class="line">      &quot;urlPrefix&quot;: &quot;http://10.168.107.12:80/scheduler&quot;,</span><br><span class="line">      &quot;filterVerb&quot;: &quot;predicates/always_true&quot;,</span><br><span class="line">      &quot;prioritizeVerb&quot;: &quot;priorities/zero_score&quot;,</span><br><span class="line">      &quot;preemptVerb&quot;: &quot;preemption&quot;,</span><br><span class="line">      &quot;bindVerb&quot;: &quot;&quot;,</span><br><span class="line">      &quot;weight&quot;: 1,</span><br><span class="line">      &quot;enableHttps&quot;: false,</span><br><span class="line">      &quot;nodeCacheCapable&quot;: false</span><br><span class="line">    &#125;],</span><br><span class="line">    &quot;hardPodAffinitySymmetricWeight&quot; : 10</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>创建一个名为 my-scheduler-config 的 configmaps，该配置文件应该包含一个 KubeSchedulerConfiguration 对象，data 下的 config.yaml 文件指定了调度器的一些参数，包括 leader 选举，调度算法策略的选择（指定另一个 configmaps），以及指定调度器的名称为 my-kube-scheduler。</p>
<p>相应的创建一个 my-scheduler-policy 的 configmaps，里面指定了选择哪些预选、优选策略，以及外部扩展调度程序的 urlPrefix、扩展预选 URI、扩展优选 URI、扩展 pod 优先级抢占 URI、扩展 bind URI、扩展优选算法的权重等，可以根据需求进行调整。</p>
<p>2、yaml 文件中将 configmaps：my-scheduler-config 以文件的形式挂载到容器内 &#x2F; my-scheduler 目录下，并在启动参数中指定–config&#x3D;&#x2F;my-scheduler&#x2F;config.yaml，启动 kube-scheduler。</p>
<h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><p>k8s 中 kube-scheuler 的高可用是通过 leaderElection 实现的，一般三个 master，哪一个先起来就是 leader, 虽然两台机器上都安装了 scheduler, 但是只有 leader 提供服务, 另外两个上面的 scheduler 是处于等待状态, 并没有真正运行自己的逻辑。</p>
<p>当 leader 异常后，其他的 scheduler 服务就会成为 leaeder，继续提供服务。</p>
<p>当我们部署多个调度器的时候，每个调度器都会各自调度属于自己的 pod。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kingjcy.github.io/media/cloud/k8s/scheduler"></p>
<p>核心：待调度的 pod 列表、可有的合适的 node 列表、调度算法和策略。</p>
<p>1、首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源</p>
<p>2、API Server 收到用户请求后，存储相关数据到 etcd 数据库中</p>
<p>3、watch apiserver，将 spec.nodeName 为空的 Pod 放入调度器内部的调度队列中</p>
<p>4、调度器监听 API Server 查看为调度 (bind) 的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点</p>
<ul>
<li>从调度队列中 Pop 出一个 Pod，开始一个标准的调度周期</li>
<li>预选阶段 (Predicates)，过滤节点，调度器用一组规则过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉</li>
<li>优选阶段 (Priorities)，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，调度器会考虑一些整体的优化策略，比如把 Deployment 控制的多个 Pod 副本分布到不同的主机上，使用最低负载的主机等等策略</li>
</ul>
<p>4、经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 binding 操作，然后将结果存储到 etcd 中</p>
<p>5、最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作</p>
<h2 id="预选策略"><a href="#预选策略" class="headerlink" title="预选策略"></a>预选策略</h2><p>我们在部署应用的时候，如果发现有 Pod 一直处于 Pending 状态，那么就是没有满足调度条件的节点，这个时候可以去检查下节点资源是否可用。</p>
<p>在 Kubernetes 中，默认的调度策略有如下三种。</p>
<h3 id="GeneralPredicates"><a href="#GeneralPredicates" class="headerlink" title="GeneralPredicates"></a>GeneralPredicates</h3><p>第一种类型，叫作 GeneralPredicates。顾名思义，这一组过滤规则，负责的是最基础的调度策略。比如，PodFitsResources 计算的就是宿主机的 CPU 和内存资源等是否够用。</p>
<blockquote>
<p>podFistResources：资源要求</p>
</blockquote>
<p>判断备选节点资源是否满足备选 pod 的需求，检测过程如下：</p>
<ul>
<li>计算备选 pod 和节点中已存在的 pod 的所有容器的需求资源（CPU 和内存）的总和</li>
<li>获得备选节点的状态信息，其中包括节点的资源信息</li>
<li>如果备选 pod 和节点中已存在 pod 的所有容器的需求资源（CPU 和内存）的总和超出了备选节点拥有的资源，则返回 false，表明备选节点不适合备选 pod，否则返回 true, 表明备选节点适合备选 pod</li>
</ul>
<blockquote>
<p>PodSelectorMatches：标签匹配</p>
</blockquote>
<p>判断备选节点是否包含备选 pod 的标签选择器指定的标签：</p>
<ul>
<li>如果 pod 没有指定 spec.nodeSelector 标签选择器，则返回 true</li>
<li>如果获得备选节点的标签信息，判断节点是否包含备选 pod 的标签选择器所指的标签，如果包含返回 true，不包含返回 false</li>
</ul>
<blockquote>
<p>PodFitsHost</p>
</blockquote>
<p>判断备选 pod 的 spec.nodeName 域所指定的节点名称和备选节点的名称是否一致，如果一致返回 true，否则返回 false。</p>
<blockquote>
<p>PodFitsPorts</p>
</blockquote>
<p>判断备选 pod 所用的端口列表汇中的端口是否在备选节点中被占用，如果被占用，则返回 false，否则返回 true。</p>
<blockquote>
<p>PodFitsHostPorts</p>
</blockquote>
<p>节点上已经使用的 port 是否和 Pod 申请的 port 冲突</p>
<h3 id="Volume-相关"><a href="#Volume-相关" class="headerlink" title="Volume 相关"></a>Volume 相关</h3><p>第二种类型，是与 Volume 相关的过滤规则。这一组过滤规则，负责的是跟容器持久化 Volume 相关的调度策略。</p>
<blockquote>
<p>NoDiskconflict：磁盘冲突</p>
</blockquote>
<p>判断备选 pod 的 gcePersistentDisk 或者 AWSElasticBlockStore 和备选的节点中已存在的 pod 是否存在冲突具体检测过程如下：</p>
<ul>
<li>首先，读取备选 pod 的所有的 volume 信息，对每一个 volume 执行一下步骤的冲突检测</li>
<li>如果该 volume 是 gcePersistentDisk，则将 volume 和备选节点上的所有 pod 的每个 volume 进行比较，如果发现相同的 gcePersistentDisk，则返回 false，表明磁盘冲突，检测结束，反馈给调度器该备选节点不合适作为备选的 pod，如果 volume 是 AWSElasticBlockStore，则将 volume 和备选节点上的所有 pod 的每个 volume 进行比较，如果发现相同的 AWSElasticBlockStore，则返回 false，表明磁盘冲突，检测结束，反馈给调度器该备选节点不合适作为备选的 pod</li>
<li>最终，检查备选 pod 的所有的 volume 均为发现冲突，则返回 true，表明不存在磁盘冲突，反馈给调度器该备选节点合适备选 pod</li>
</ul>
<blockquote>
<p>MaxPDVolumeCountPredicate</p>
</blockquote>
<p>MaxPDVolumeCountPredicate 检查的条件，则是一个节点上某种类型的持久化 Volume 是不是已经超过了一定数目，如果是的话，那么声明使用该类型持久化 Volume 的 Pod 就不能再调度到这个节点了。</p>
<blockquote>
<p>VolumeZonePredicate</p>
</blockquote>
<p>VolumeZonePredicate，则是检查持久化 Volume 的 Zone（高可用域）标签，是否与待考察节点的 Zone 标签相匹配。此外，这里还有一个叫作 VolumeBindingPredicate 的规则。它负责检查的，是该 Pod 对应的 PV 的 nodeAffinity 字段，是否跟某个节点的标签相匹配。</p>
<h3 id="宿主机相关"><a href="#宿主机相关" class="headerlink" title="宿主机相关"></a>宿主机相关</h3><p>第三种类型，是宿主机相关的过滤规则。这一组规则，主要考察待调度 Pod 是否满足 Node 本身的某些条件。比如，PodToleratesNodeTaints，负责检查的就是我们前面经常用到的 Node 的 “污点” 机制。只有当 Pod 的 Toleration 字段与 Node 的 Taint 字段能够匹配的时候，这个 Pod 才能被调度到该节点上。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>Predicates 过滤有一系列的算法可以使用，上面就是简单的列举几个，还有很多，更多更详细的我们可以查看源码文件：k8s.io&#x2F;kubernetes&#x2F;pkg&#x2F;scheduler&#x2F;algorithm&#x2F;predicates&#x2F;predicates.go。</p>
<p>虽然 Predicates 是串行的，但是当开始调度一个 Pod 时，Kubernetes 调度器会同时启动 16 个 Goroutine，来并发地为集群里的所有 Node 计算 Predicates，最后返回可以运行这个 Pod 的宿主机列表。</p>
<h2 id="优选策略"><a href="#优选策略" class="headerlink" title="优选策略"></a>优选策略</h2><p>在 Predicates 阶段完成了节点的 “过滤” 之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。Priorities 里最常用到的一个打分规则，是 LeastRequestedPriority。</p>
<h3 id="leastRequestedPriority"><a href="#leastRequestedPriority" class="headerlink" title="leastRequestedPriority"></a>leastRequestedPriority</h3><p>该策略用于从备选节点列表中选出资源消耗最小的节点：</p>
<ul>
<li>计算出所有备选节点上运行的 pod 和备选 pod 的 CPU 占用量</li>
<li>计算出所有备选节点上运行的 pod 和备选 pod 的 memory 占用量</li>
<li>根据特定的算法，计算每个节点的得分</li>
</ul>
<p>公式如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2</span><br></pre></td></tr></table></figure>

<p>实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。</p>
<h3 id="CalculateNodeLabelPriority"><a href="#CalculateNodeLabelPriority" class="headerlink" title="CalculateNodeLabelPriority"></a>CalculateNodeLabelPriority</h3><p>如果用户在配置中指定了该策略，则 scheduler 会通过 registerCustomPriorityFunction 方法注册该策略。该策略用于判断策略列出的标签在备选节点中存在时，是否选择该备选节点。如果备选节点的标签在优选策略的标签列表中且优选策略的 presence 值为 true，或者备选节点的标签不在优选策略的标签列表中且优选策略的 presence 值为 false，则备选节点 score&#x3D;10，否则等于 0。</p>
<h3 id="BalancedResourceAllocation"><a href="#BalancedResourceAllocation" class="headerlink" title="BalancedResourceAllocation"></a>BalancedResourceAllocation</h3><p>该优选策略用于从备选节点列表中选出各项资源使用率最均衡的节点：</p>
<ul>
<li>计算出所有备选节点上运行的 pod 和备选 pod 的 CPU 占用量</li>
<li>计算出所有备选节点上运行的 pod 和备选 pod 的 memory 占用量</li>
<li>根据特定的算法，计算每个节点的得分</li>
</ul>
<p>公式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10</span><br></pre></td></tr></table></figure>

<p>Pod 请求的资源 &#x2F; 节点上的可用资源。而 variance 算法的作用，则是计算每两种资源 Fraction 之间的 “距离”。而最后选择的，则是资源 Fraction 差距最小的节点。所以说，BalancedResourceAllocation 选择的，其实是调度完成后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况。</p>
<h3 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h3><p>还有很多其他的策略，比如如下等等</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SelectorSpreadPriority：为了更好的高可用，对同属于一个 Deployment 或者 RC 下面的多个 Pod 副本，尽量调度到多个不同的节点上，当一个 Pod 被调度的时候，会先去查找该 Pod 对应的 controller，然后查看该 controller 下面的已存在的 Pod，运行 Pod 越少的节点权重越高</span><br><span class="line">ImageLocalityPriority：就是如果在某个节点上已经有要使用的镜像节点了，镜像总大小值越大，权重就越高</span><br><span class="line">NodeAffinityPriority：这个就是根据节点的亲和性来计算一个权重值，后面我们会详细讲解亲和性的使用方法</span><br></pre></td></tr></table></figure>

<p>同样我们可以查看源码文件：k8s.io&#x2F;kubernetes&#x2F;pkg&#x2F;scheduler&#x2F;algorithm&#x2F;priorities&#x2F; 了解更多信息。每一个优先级函数会返回一个 0-10 的分数，分数越高表示节点越优，同时每一个函数也会对应一个表示权重的值。最终主机的得分用以下公式计算得出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + … + (weightn * priorityFuncn)</span><br></pre></td></tr></table></figure>

<p>priority functions 集合中的每一个函数都有一个权重 (weight)，最终的值为 weight 和 priority functions 的乘积，而一个节点的 weight 就是所有 priority functions 结果的加和</p>
<h2 id="亲和性"><a href="#亲和性" class="headerlink" title="亲和性"></a>亲和性</h2><p>在我们实际使用中，有着很多的使用的地方，比如指定 node，屏蔽 node。</p>
<h3 id="指定-Node-节点调度"><a href="#指定-Node-节点调度" class="headerlink" title="指定 Node 节点调度"></a>指定 Node 节点调度</h3><p>有三种方式指定 Pod 只运行在指定的 Node 节点上</p>
<ul>
<li>nodeSelector：只调度到匹配指定 label 的 Node 上</li>
<li>nodeAffinity：功能更丰富的 Node 选择器，比如支持集合操作</li>
<li>podAffinity：调度到满足条件的 Pod 所在的 Node 上</li>
</ul>
<p>1、nodeSelector</p>
<p>首先给 Node 打上标签</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl label nodes node-01 disktype=ssd</span><br></pre></td></tr></table></figure>

<p>然后在 daemonset 中指定 nodeSelector 为 disktype&#x3D;ssd：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  nodeSelector:</span><br><span class="line">    disktype: ssd</span><br></pre></td></tr></table></figure>

<p>根据默认调度器的 PodSelectorMatches 策略选出合适的节点，然后打分进行分配。</p>
<p>2、nodeAffinity</p>
<p>nodeAffinity 目前支持两种：requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution，分别代表必须满足条件和优选条件。比如下面的例子代表调度到包含标签 kubernetes.io&#x2F;e2e-az-name 并且值为 e2e-az1 或 e2e-az2 的 Node 上，并且优选还带有标签 another-node-label-key&#x3D;another-node-label-value 的 Node。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-node-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: kubernetes.io/e2e-az-name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - e2e-az1</span><br><span class="line">            - e2e-az2</span><br><span class="line">      preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">      - weight: 1</span><br><span class="line">        preference:</span><br><span class="line">          matchExpressions:</span><br><span class="line">          - key: another-node-label-key</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - another-node-label-value</span><br><span class="line">  containers:</span><br><span class="line">  - name: with-node-affinity</span><br><span class="line">    image: gcr.io/google_containers/pause:2.0</span><br></pre></td></tr></table></figure>

<p>3、podAffinity</p>
<p>podAffinity 基于 Pod 的标签来选择 Node，仅调度到满足条件 Pod 所在的 Node 上，支持 podAffinity 和 podAntiAffinity。这个功能比较绕，以下面的例子为例：</p>
<p>如果一个 “Node 所在 Zone 中包含至少一个带有 security&#x3D;S1 标签且运行中的 Pod”，那么可以调度到该 Node</p>
<p>不调度到 “包含至少一个带有 security&#x3D;S2 标签且运行中 Pod” 的 Node 上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-pod-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    podAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">      - labelSelector:</span><br><span class="line">          matchExpressions:</span><br><span class="line">          - key: security</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - S1</span><br><span class="line">        topologyKey: failure-domain.beta.kubernetes.io/zone</span><br><span class="line">    podAntiAffinity:</span><br><span class="line">      preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">      - weight: 100</span><br><span class="line">        podAffinityTerm:</span><br><span class="line">          labelSelector:</span><br><span class="line">            matchExpressions:</span><br><span class="line">            - key: security</span><br><span class="line">              operator: In</span><br><span class="line">              values:</span><br><span class="line">              - S2</span><br><span class="line">          topologyKey: kubernetes.io/hostname</span><br><span class="line">  containers:</span><br><span class="line">  - name: with-pod-affinity</span><br><span class="line">    image: gcr.io/google_containers/pause:2.0</span><br></pre></td></tr></table></figure>

<p>可以看出基本上都是根据 label 进行调度选择。</p>
<h3 id="亲和性（Affinity）与非亲和性（anti-affinity）"><a href="#亲和性（Affinity）与非亲和性（anti-affinity）" class="headerlink" title="亲和性（Affinity）与非亲和性（anti-affinity）"></a>亲和性（Affinity）与非亲和性（anti-affinity）</h3><p>在上面会说亲和性的操作更加多样化，所以我们一般都是使用亲和性而不是使用 selector 在调度中。亲和性主要分为 3 种类型：node affinity 与 inter-pod affinity&#x2F;anti-affinity</p>
<ul>
<li>node affinity 我们在 deployment 的调度的时候说明过亲和性调度的使用，主要是两个策略：强制和优先。调度到符合条件的 node 上去。</li>
<li>pod Affinity 用于调度 pod 可以和哪些 pod 部署在同一拓扑结构之下。</li>
<li>podAntiAffinity 相反，其用于规定 pod 不可以和哪些 pod 部署在同一拓扑结构下。AntiAffinity 同样有两个策略：强制和优先。</li>
</ul>
<h3 id="屏蔽-Node-节点调度"><a href="#屏蔽-Node-节点调度" class="headerlink" title="屏蔽 Node 节点调度"></a>屏蔽 Node 节点调度</h3><p>Taints 和 tolerations 用于保证 Pod 不被调度到不合适的 Node 上，其中 Taint 应用于 Node 上，而 toleration 则应用于 Pod 上。</p>
<p>目前支持的 taint 类型</p>
<ul>
<li>NoSchedule：新的 Pod 不调度到该 Node 上，不影响正在运行的 Pod</li>
<li>PreferNoSchedule：soft 版的 NoSchedule，尽量不调度到该 Node 上</li>
<li>NoExecute：新的 Pod 不调度到该 Node 上，并且删除（evict）已在运行的 Pod。Pod 可以增加一个时间（tolerationSeconds），</li>
</ul>
<p>然而，当 Pod 的 Tolerations 匹配 Node 的所有 Taints 的时候可以调度到该 Node 上；当 Pod 是已经运行的时候，也不会被删除（evicted）。另外对于 NoExecute，如果 Pod 增加了一个 tolerationSeconds，则会在该时间之后才删除 Pod。</p>
<p>比如，假设 node1 上应用以下几个 taint</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes node1 key1=value1:NoSchedule</span><br><span class="line">kubectl taint nodes node1 key1=value1:NoExecute</span><br><span class="line">kubectl taint nodes node1 key2=value2:NoSchedule</span><br></pre></td></tr></table></figure>

<p>下面的这个 Pod 由于没有 toleratekey2&#x3D;value2:NoSchedule 无法调度到 node1 上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tolerations:</span><br><span class="line">- key: &quot;key1&quot;</span><br><span class="line">  operator: &quot;Equal&quot;</span><br><span class="line">  value: &quot;value1&quot;</span><br><span class="line">  effect: &quot;NoSchedule&quot;</span><br><span class="line">- key: &quot;key1&quot;</span><br><span class="line">  operator: &quot;Equal&quot;</span><br><span class="line">  value: &quot;value1&quot;</span><br><span class="line">  effect: &quot;NoExecute&quot;</span><br></pre></td></tr></table></figure>

<p>而正在运行且带有 tolerationSeconds 的 Pod 则会在 600s 之后删除</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tolerations:</span><br><span class="line">- key: &quot;key1&quot;</span><br><span class="line">  operator: &quot;Equal&quot;</span><br><span class="line">  value: &quot;value1&quot;</span><br><span class="line">  effect: &quot;NoSchedule&quot;</span><br><span class="line">- key: &quot;key1&quot;</span><br><span class="line">  operator: &quot;Equal&quot;</span><br><span class="line">  value: &quot;value1&quot;</span><br><span class="line">  effect: &quot;NoExecute&quot;</span><br><span class="line">  tolerationSeconds: 600</span><br><span class="line">- key: &quot;key2&quot;</span><br><span class="line">  operator: &quot;Equal&quot;</span><br><span class="line">  value: &quot;value2&quot;</span><br><span class="line">  effect: &quot;NoSchedule&quot;</span><br></pre></td></tr></table></figure>

<p>注意，DaemonSet 创建的 Pod 会自动加上对 node.alpha.kubernetes.io&#x2F;unreachable 和 node.alpha.kubernetes.io&#x2F;notReady 的 NoExecute Toleration，以避免它们因此被删除。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kingjcy.github.io/media/cloud/k8s/scheduler.jpg"></p>
<p>可以看到，Kubernetes 的调度器的核心，实际上就是两个相互独立的控制循环。</p>
<blockquote>
<p>Informer Path</p>
</blockquote>
<p>第一个控制循环，我们可以称之为 Informer Path。它的主要目的，是启动一系列 Informer，用来监听（Watch）Etcd 中 Pod、Node、Service 等与调度相关的 API 对象的变化。比如，当一个待调度 Pod（即：它的 nodeName 字段是空的）被创建出来之后，调度器就会通过 Pod Informer 的 Handler，将这个待调度 Pod 添加进调度队列。在默认情况下，Kubernetes 的调度队列是一个 PriorityQueue（优先级队列），并且当某些集群信息发生变化的时候，调度器还会对调度队列里的内容进行一些特殊操作。这里的设计，主要是出于调度优先级和抢占的考虑，我会在后面再详细介绍这部分内容。此外，Kubernetes 的默认调度器还要负责对调度器缓存（即：scheduler cache）进行更新。事实上，Kubernetes 调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息 Cache 化，以便从根本上提高 Predicate 和 Priority 调度算法的执行效率。</p>
<blockquote>
<p>Scheduling Path</p>
</blockquote>
<p>第二个控制循环，是调度器负责 Pod 调度的主循环，我们可以称之为 Scheduling Path。Scheduling Path 的主要逻辑，就是不断地从调度队列里出队一个 Pod。然后，调用 Predicates 算法进行 “过滤”。这一步“过滤” 得到的一组 Node，就是所有可以运行这个 Pod 的宿主机列表。当然，Predicates 算法需要的 Node 信息，都是从 Scheduler Cache 里直接拿到的，这是调度器保证算法执行效率的主要手段之一。接下来，调度器就会再调用 Priorities 算法为上述列表里的 Node 打分，分数从 0 到 10。得分最高的 Node，就会作为这次调度的结果。其实这就是我们上面的说的调度过程。也是我们核心关注的地方，后面的调度框架也是这部分的扩展。</p>
<p>调度算法执行完成后，调度器就需要将 Pod 对象的 nodeName 字段的值，修改为上述 Node 的名字。这个步骤在 Kubernetes 里面被称作 Bind。但是，为了不在关键调度路径里远程访问 APIServer，Kubernetes 的默认调度器在 Bind 阶段，只会更新 Scheduler Cache 里的 Pod 和 Node 的信息。这种基于 “乐观” 假设的 API 对象更新方式，在 Kubernetes 里被称作 Assume。Assume 之后，调度器才会创建一个 Goroutine 来异步地向 APIServer 发起更新 Pod 的请求，来真正完成 Bind 操作。如果这次异步的 Bind 过程失败了，其实也没有太大关系，等 Scheduler Cache 同步之后一切就会恢复正常。当然，正是由于上述 Kubernetes 调度器的 “乐观” 绑定的设计，当一个新的 Pod 完成调度需要在某个节点上运行起来之前，该节点上的 kubelet 还会通过一个叫作 Admit 的操作来再次验证该 Pod 是否确实能够运行在该节点上。这一步 Admit 操作，实际上就是把一组叫作 GeneralPredicates 的、最基本的调度算法，比如：“资源是否可用”“端口是否冲突”等再执行一遍，作为 kubelet 端的二次确认。</p>
<p>除了上述的 “Cache 化” 和“乐观绑定”，Kubernetes 默认调度器还有一个重要的设计，那就是“无锁化”。在 Scheduling Path 上，调度器会启动多个 Goroutine 以节点为粒度并发执行 Predicates 算法，从而提高这一阶段的执行效率。而与之类似的，Priorities 算法也会以 MapReduce 的方式并行计算然后再进行汇总。而在这些所有需要并发的路径上，调度器会避免设置任何全局的竞争资源，从而免去了使用锁进行同步带来的巨大的性能损耗。所以，在这种思想的指导下，如果你再去查看一下前面的调度器原理图，你就会发现，Kubernetes 调度器只有对调度队列和 Scheduler Cache 进行操作时，才需要加锁。而这两部分操作，都不在 Scheduling Path 的算法执行路径上。</p>
<p>Kubernetes 调度器的上述设计思想，也是在集群规模不断增长的演进过程中逐步实现的。尤其是 “Cache 化”，这个变化其实是最近几年 Kubernetes 调度器性能得以提升的一个关键演化。</p>
<h2 id="优先级调度"><a href="#优先级调度" class="headerlink" title="优先级调度"></a>优先级调度</h2><p>Kubernetes 规定，优先级是一个 32 bit 的整数，最大值不超过 1000000000（10 亿，1 billion），并且值越大代表优先级越高。而超出 10 亿的值，其实是被 Kubernetes 保留下来分配给系统 Pod 使用的，而对于没有声明 PriorityClass 的 Pod 来说，它们的优先级就是 0。</p>
<p>Pod 的优先级，高优先级的 Pod 会优先被调度，或者在资源不足低情况牺牲低优先级的 Pod，以便于重要的 Pod 能够得到资源部署。</p>
<p>要定义 Pod 优先级，就需要先定义 PriorityClass 对象，该对象没有 Namespace 的限制：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PriorityClass</span><br><span class="line">metadata:</span><br><span class="line">  name: high-priority</span><br><span class="line">value: 1000000</span><br><span class="line">globalDefault: false</span><br><span class="line">description: &quot;This priority class should be used for XYZ service pods only.&quot;</span><br></pre></td></tr></table></figure>

<p>其中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">value为 32 位整数的优先级，该值越大，优先级越高</span><br><span class="line">globalDefault用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个PriorityClass将其设置为 true</span><br></pre></td></tr></table></figure>

<p>然后通过在 Pod 的 spec.priorityClassName 中指定已定义的 PriorityClass 名称即可：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  priorityClassName: high-priority</span><br></pre></td></tr></table></figure>

<p>另外一个值得注意的是当节点没有足够的资源供调度器调度 Pod，导致 Pod 处于 pending 时，抢占（preemption）逻辑就会被触发。Preemption 会尝试从一个节点删除低优先级的 Pod，从而释放资源使高优先级的 Pod 得到节点资源进行部署。</p>
<h3 id="抢占式调度"><a href="#抢占式调度" class="headerlink" title="抢占式调度"></a>抢占式调度</h3><p>而当一个高优先级的 Pod 调度失败的时候，调度器的抢占能力就会被触发。这时，调度器就会试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级 Pod 被删除后，待调度的高优先级 Pod 就可以被调度到这个节点上。</p>
<p>当上述抢占过程发生时，抢占者并不会立刻被调度到被抢占的 Node 上。事实上，调度器只会将抢占者的 spec.nominatedNodeName 字段，设置为被抢占的 Node 的名字。然后，抢占者会重新进入下一个调度周期，然后在新的调度周期里来决定是不是要运行在被抢占的节点上。这当然也就意味着，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。这样设计的一个重要原因是，调度器只会通过标准的 DELETE API 来删除被抢占的 Pod，所以，这些 Pod 必然是有一定的 “优雅退出” 时间（默认是 30s）的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退出期间，集群的可调度性可能会发生的变化，把抢占者交给下一个调度周期再处理，是一个非常合理的选择。而在抢占者等待被调度的过程中，如果有其他更高优先级的 Pod 也要抢占同一个节点，那么调度器就会清空原抢占者的 spec.nominatedNodeName 字段，从而允许更高优先级的抢占者执行抢占，并且，这也就使得原抢占者本身，也有机会去重新抢占其他节点。这些，都是设置 nominatedNodeName 字段的主要目的。</p>
<p>Kubernetes 调度器里的抢占机制原理</p>
<p>Kubernetes 调度器实现抢占算法的一个最重要的设计，就是在调度队列的实现里，使用了两个不同的队列。</p>
<ul>
<li>第一个队列，叫作 activeQ。凡是在 activeQ 里的 Pod，都是下一个调度周期需要调度的对象。所以，当你在 Kubernetes 集群里新创建一个 Pod 的时候，调度器会将这个 Pod 入队到 activeQ 里面。而我在前面提到过的、调度器不断从队列里出队（Pop）一个 Pod 进行调度，实际上都是从 activeQ 里出队的。</li>
<li>第二个队列，叫作 unschedulableQ，专门用来存放调度失败的 Pod。</li>
</ul>
<p>调度失败之后，抢占者就会被放进 unschedulableQ 里面。然后，这次失败事件就会触发调度器为抢占者寻找牺牲者的流程。</p>
<ul>
<li>第一步，调度器会检查这次失败事件的原因，来确认抢占是不是可以帮助抢占者找到一个新节点。这是因为有很多 Predicates 的失败是不能通过抢占来解决的。比如，PodFitsHost 算法（负责的是，检查 Pod 的 nodeSelector 与 Node 的名字是否匹配），这种情况下，除非 Node 的名字发生变化，否则你即使删除再多的 Pod，抢占者也不可能调度成功。当遍历完所有的节点之后，调度器会在上述模拟产生的所有抢占结果里做一个选择，找出最佳结果。而这一步的判断原则，就是尽量减少抢占对整个系统的影响。比如，需要抢占的 Pod 越少越好，需要抢占的 Pod 的优先级越低越好，等等。</li>
<li>第二步，如果确定抢占可以发生，那么调度器就会把自己缓存的所有节点信息复制一份，然后使用这个副本来模拟抢占过程。在得到了最佳的抢占结果之后，这个结果里的 Node，就是即将被抢占的 Node；被删除的 Pod 列表，就是牺牲者。所以接下来，调度器就可以真正开始抢占的操作了，这个过程，可以分为三步。<ul>
<li>第一步，调度器会检查牺牲者列表，清理这些 Pod 所携带的 nominatedNodeName 字段</li>
<li>第二步，调度器会把抢占者的 nominatedNodeName，设置为被抢占的 Node 的名字。</li>
<li>第三步，调度器会开启一个 Goroutine，同步地删除牺牲者。</li>
</ul>
</li>
</ul>
<p>这里的抢占过程很容易理解。调度器会检查缓存副本里的每一个节点，然后从该节点上最低优先级的 Pod 开始，逐一 “删除” 这些 Pod。而每删除一个低优先级 Pod，调度器都会检查一下抢占者是否能够运行在该 Node 上。一旦可以运行，调度器就记录下这个 Node 的名字和被删除 Pod 的列表，这就是一次抢占过程的结果了。</p>
<p>调度器就会对这个 Node ，将同样的 Predicates 算法运行两遍。</p>
<ul>
<li>第一遍， 调度器会假设上述 “潜在的抢占者” 已经运行在这个节点上，然后执行 Predicates 算法；</li>
<li>第二遍， 调度器会正常执行 Predicates 算法，即：不考虑任何 “潜在的抢占者”。</li>
</ul>
<p>不难想到，这里需要执行第一遍 Predicates 算法的原因，是由于 InterPodAntiAffinity 规则的存在。由于 InterPodAntiAffinity 规则关心待考察节点上所有 Pod 之间的互斥关系，所以我们在执行调度算法时必须考虑，如果抢占者已经存在于待考察 Node 上时，待调度 Pod 还能不能调度成功。当然，这也就意味着，我们在这一步只需要考虑那些优先级等于或者大于待调度 Pod 的抢占者。毕竟对于其他较低优先级 Pod 来说，待调度 Pod 总是可以通过抢占运行在待考察 Node 上。而我们需要执行第二遍 Predicates 算法的原因，则是因为 “潜在的抢占者” 最后不一定会运行在待考察的 Node 上。关于这一点，我在前面已经讲解过了：Kubernetes 调度器并不保证抢占者一定会运行在当初选定的被抢占的 Node 上。</p>
<p>要编写一个优秀的调度器却不容易，因为要考虑的东西很多：</p>
<ul>
<li>尽可能地将 workload 平均到不同的节点，减少单个节点宕机造成的损失</li>
<li>可扩展性。随着集群规模的增加，怎么保证调度器不会成为性能的瓶颈</li>
<li>高可用。调度器能做组成集群，任何一个调度器出现问题，不会影响整个集群的调度</li>
<li>灵活性。不同的用户有不同的调度需求，一个优秀的调度器还要允许用户能配置不同的调度算法</li>
<li>资源合理和高效利用。调度器应该尽可能地提高集群的资源利用率，防止资源的浪费</li>
</ul>
<p>一般来说，我们有 4 种扩展 Kubernetes 调度器的方法。</p>
<ul>
<li>直接 clone 官方的 kube-scheduler 源代码，在合适的位置直接修改代码，然后重新编译运行修改后的程序，当然这种方法是最不建议使用的，也不实用，因为需要花费大量额外的精力来和上游的调度程序更改保持一致。</li>
<li>和默认的调度程序一起运行独立的调度程序，默认的调度器和我们自定义的调度器可以通过 Pod 的 spec.schedulerName 来覆盖各自的 Pod，默认是使用 default 默认的调度器，但是多个调度程序共存的情况下也比较麻烦，比如当多个调度器将 Pod 调度到同一个节点的时候，可能会遇到一些问题，因为很有可能两个调度器都同时将两个 Pod 调度到同一个节点上去，但是很有可能其中一个 Pod 运行后其实资源就消耗完了，并且维护一个高质量的自定义调度程序也不是很容易的，因为我们需要全面了解默认的调度程序，整体 Kubernetes 的架构知识以及各种 Kubernetes API 对象的各种关系或限制。</li>
<li>调度器扩展程序，这个方案目前是一个可行的方案，可以和上游调度程序兼容，所谓的调度器扩展程序其实就是一个可配置的 Webhook 而已，里面包含 过滤器 和 优先级 两个端点，分别对应调度周期中的两个主要阶段（过滤和打分）。</li>
<li>通过调度框架（Scheduling Framework），Kubernetes v1.15 版本中引入了可插拔架构的调度框架，使得定制调度器这个任务变得更加的容易。调库框架向现有的调度器中添加了一组插件化的 API，该 API 在保持调度程序 “核心” 简单且易于维护的同时，使得大部分的调度功能以插件的形式存在，而且在我们现在的 v1.16 版本中上面的 调度器扩展程序 也已经被废弃了，所以以后调度框架才是自定义调度器的核心方式。</li>
</ul>
<p>这里我们可以简单介绍下后面三种方式的实现。</p>
<h2 id="调度器扩展程序"><a href="#调度器扩展程序" class="headerlink" title="调度器扩展程序"></a>调度器扩展程序</h2><p>1、实现扩展程序</p>
<p>我们直接用 golang 来实现一个简单的调度器扩展程序，当然你可以使用其他任何编程语言，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">    router := httprouter.New()</span><br><span class="line">    router.GET(&quot;/&quot;, Index)</span><br><span class="line">    router.POST(&quot;/filter&quot;, Filter)</span><br><span class="line">    router.POST(&quot;/prioritize&quot;, Prioritize)</span><br><span class="line"></span><br><span class="line">    log.Fatal(http.ListenAndServe(&quot;:8888&quot;, router))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后接下来我们需要实现 &#x2F;filter 和 &#x2F;prioritize 两个端点的处理程序。</p>
<p>其中 Filter 这个扩展函数接收一个输入类型为 schedulerapi.ExtenderArgs 的参数，然后返回一个类型为 *schedulerapi.ExtenderFilterResult 的值。在函数中，我们可以进一步过滤输入的节点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// filter 根据扩展程序定义的预选规则来过滤节点</span><br><span class="line">func filter(args schedulerapi.ExtenderArgs) *schedulerapi.ExtenderFilterResult &#123;</span><br><span class="line">    var filteredNodes []v1.Node</span><br><span class="line">    failedNodes := make(schedulerapi.FailedNodesMap)</span><br><span class="line">    pod := args.Pod</span><br><span class="line"></span><br><span class="line">    for _, node := range args.Nodes.Items &#123;</span><br><span class="line">        fits, failReasons, _ := podFitsOnNode(pod, node)</span><br><span class="line">        if fits &#123;</span><br><span class="line">            filteredNodes = append(filteredNodes, node)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            failedNodes[node.Name] = strings.Join(failReasons, &quot;,&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    result := schedulerapi.ExtenderFilterResult&#123;</span><br><span class="line">        Nodes: &amp;v1.NodeList&#123;</span><br><span class="line">            Items: filteredNodes,</span><br><span class="line">        &#125;,</span><br><span class="line">        FailedNodes: failedNodes,</span><br><span class="line">        Error:       &quot;&quot;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return &amp;result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在过滤函数中，我们循环每个节点然后用我们自己实现的业务逻辑来判断是否应该批准该节点，这里我们实现比较简单，在 podFitsOnNode() 函数中我们只是简单的检查随机数是否为偶数来判断即可，如果是的话我们就认为这是一个幸运的节点，否则拒绝批准该节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">var predicatesSorted = []string&#123;LuckyPred&#125;</span><br><span class="line"></span><br><span class="line">var predicatesFuncs = map[string]FitPredicate&#123;</span><br><span class="line">    LuckyPred: LuckyPredicate,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type FitPredicate func(pod *v1.Pod, node v1.Node) (bool, []string, error)</span><br><span class="line"></span><br><span class="line">func podFitsOnNode(pod *v1.Pod, node v1.Node) (bool, []string, error) &#123;</span><br><span class="line">    fits := true</span><br><span class="line">    var failReasons []string</span><br><span class="line">    for _, predicateKey := range predicatesSorted &#123;</span><br><span class="line">        fit, failures, err := predicatesFuncs[predicateKey](pod, node)</span><br><span class="line">        if err != nil &#123;</span><br><span class="line">            return false, nil, err</span><br><span class="line">        &#125;</span><br><span class="line">        fits = fits &amp;&amp; fit</span><br><span class="line">        failReasons = append(failReasons, failures...)</span><br><span class="line">    &#125;</span><br><span class="line">    return fits, failReasons, nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func LuckyPredicate(pod *v1.Pod, node v1.Node) (bool, []string, error) &#123;</span><br><span class="line">    lucky := rand.Intn(2) == 0</span><br><span class="line">    if lucky &#123;</span><br><span class="line">        log.Printf(&quot;pod %v/%v is lucky to fit on node %v\n&quot;, pod.Name, pod.Namespace, node.Name)</span><br><span class="line">        return true, nil, nil</span><br><span class="line">    &#125;</span><br><span class="line">    log.Printf(&quot;pod %v/%v is unlucky to fit on node %v\n&quot;, pod.Name, pod.Namespace, node.Name)</span><br><span class="line">    return false, []string&#123;LuckyPredFailMsg&#125;, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>同样的打分功能用同样的方式来实现，我们在每个节点上随机给出一个分数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// it&#x27;s webhooked to pkg/scheduler/core/generic_scheduler.go#PrioritizeNodes()</span><br><span class="line">// 这个函数输出的分数会被添加会默认的调度器</span><br><span class="line">func prioritize(args schedulerapi.ExtenderArgs) *schedulerapi.HostPriorityList &#123;</span><br><span class="line">    pod := args.Pod</span><br><span class="line">    nodes := args.Nodes.Items</span><br><span class="line"></span><br><span class="line">    hostPriorityList := make(schedulerapi.HostPriorityList, len(nodes))</span><br><span class="line">    for i, node := range nodes &#123;</span><br><span class="line">        score := rand.Intn(schedulerapi.MaxPriority + 1)  // 在最大优先级内随机取一个值</span><br><span class="line">        log.Printf(luckyPrioMsg, pod.Name, pod.Namespace, score)</span><br><span class="line">        hostPriorityList[i] = schedulerapi.HostPriority&#123;</span><br><span class="line">            Host:  node.Name,</span><br><span class="line">            Score: score,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return &amp;hostPriorityList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后我们可以使用下面的命令来编译打包我们的应用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ GOOS=linux GOARCH=amd64 go build -o app</span><br></pre></td></tr></table></figure>

<p>获得我们的扩展调度器扩展程序的二进制文件 app，构建完成后，将应用 app 拷贝到 kube-scheduler 所在的节点直接运行即可，当然也可以使用 pod 运行，复制一个调度器的 YAML 文件然后更改下 schedulerName 来部署，这样就不会影响默认的调度器了，然后在需要使用这个测试的调度器的 Pod 上面指定 spec.schedulerName 即可。这就是第二种方法的多调度器。</p>
<p>2、注册扩展程序</p>
<p>我们只要在策略的配置文件中注册就可以格式是</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Policy</span><br><span class="line">extenders:</span><br><span class="line">- urlPrefix: &quot;http://127.0.0.1:8888/&quot;</span><br><span class="line">  filterVerb: &quot;filter&quot;</span><br><span class="line">  prioritizeVerb: &quot;prioritize&quot;</span><br><span class="line">  weight: 1</span><br><span class="line">  enableHttps: false</span><br></pre></td></tr></table></figure>

<p>将其写到默认的 policy 文件中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;kind&quot;: &quot;Policy&quot;,</span><br><span class="line">    &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">    &quot;predicates&quot;: [&#123;</span><br><span class="line">        &quot;name&quot;: &quot;MatchNodeSelector&quot;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        &quot;name&quot;: &quot;PodFitsResources&quot;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        &quot;name&quot;: &quot;PodFitsHostPorts&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;name&quot;: &quot;HostName&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;priorities&quot;: [&#123;</span><br><span class="line">        &quot;name&quot;: &quot;EqualPriority&quot;,</span><br><span class="line">        &quot;weight&quot;: 2</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        &quot;name&quot;: &quot;ImageLocalityPriority&quot;,</span><br><span class="line">        &quot;weight&quot;: 4</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        &quot;name&quot;: &quot;LeastRequestedPriority&quot;,</span><br><span class="line">        &quot;weight&quot;: 2</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        &quot;name&quot;: &quot;BalancedResourceAllocation&quot;,</span><br><span class="line">        &quot;weight&quot;: 2</span><br><span class="line">    &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;extenders&quot;: [&#123;</span><br><span class="line">        &quot;urlPrefix&quot;: &quot;http://127.0.0.1:8888/prefix&quot;,</span><br><span class="line">        &quot;filterVerb&quot;: &quot;filter&quot;,</span><br><span class="line">        &quot;prioritizeVerb&quot;: &quot;prioritize&quot;,</span><br><span class="line">        &quot;weight&quot;: 1,</span><br><span class="line">        &quot;bindVerb&quot;: &quot;bind&quot;,</span><br><span class="line">        &quot;enableHttps&quot;: false</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后启动 kube-scheduler 就行。这时候我们在调度的是时候，是重上到下，这样在过滤和打分阶段结束后，可以将结果分别传递给该扩展程序的端点，可以进一步过滤并确定优先级，以适应我们的特定业务需求。同一个调度器就是这样，多个调度器时候，每一个调度器都是这么个重下倒下过滤调度的流程。</p>
<p>3、验证</p>
<p>现在我们来运行一个 Deployment 查看其工作原理，我们准备一个包含 20 个副本的部署 Yaml：(test-scheduler.yaml)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: pause</span><br><span class="line">spec:</span><br><span class="line">  replicas: 20</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: pause</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: pause</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: pause</span><br><span class="line">        image: gcr.azk8s.cn/google_containers/pause:3.1</span><br></pre></td></tr></table></figure>

<p>直接创建上面的资源对象：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kuectl apply -f test-scheduler.yaml</span><br><span class="line">deployment.apps/pause created</span><br></pre></td></tr></table></figure>

<p>这个时候我们去查看下我们编写的调度器扩展程序日志：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./app</span><br><span class="line">......</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is unlucky to fit on node ydzs-node1</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 7</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-bwn7t/default is lucky to get score 9</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node3</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is unlucky to fit on node ydzs-node4</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node1</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to fit on node ydzs-node2</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 4</span><br><span class="line">2020/01/03 12:27:29 pod pause-58584fbc95-86w92/default is lucky to get score 8</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>我们可以看到 Pod 调度的过程，另外默认调度程序会定期重试失败的 Pod，因此它们将一次又一次地重新传递到我们的调度扩展程序上，我们的逻辑是检查随机数是否为偶数，所以最终所有 Pod 都将处于运行状态。</p>
<p>调度器扩展程序可能是在一些情况下可以满足我们的需求，但是他仍然有一些限制和缺点：</p>
<ul>
<li>通信成本：数据在默认调度程序和调度器扩展程序之间以 http（s）传输，在执行序列化和反序列化的时候有一定成本</li>
<li>有限的扩展点：扩展程序只能在某些阶段的末尾参与，例如 “Filter” 和“ Prioritize”，它们不能在任何阶段的开始或中间被调用</li>
<li>减法优于加法：与默认调度程序传递的节点候选列表相比，我们可能有一些需求需要添加新的候选节点列表，但这是比较冒险的操作，因为不能保证新节点可以通过其他要求，所以，调度器扩展程序最好执行 “减法”（进一步过滤），而不是 “加法”（添加节点）</li>
<li>缓存共享：上面只是一个简单的测试示例，但在真实的项目中，我们是需要通过查看整个集群的状态来做出调度决策的，默认调度程序可以很好地调度决策，但是无法共享其缓存，这意味着我们必须构建和维护自己的缓存</li>
</ul>
<p>由于这些局限性，Kubernetes 调度小组就提出了上面第四种方法来进行更好的扩展，也就是调度框架（Scheduler Framework），它基本上可以解决我们遇到的所有难题，现在也已经成官方推荐的扩展方式，所以这将是以后扩展调度器的最主流的方式。</p>
<h2 id="多调度器"><a href="#多调度器" class="headerlink" title="多调度器"></a>多调度器</h2><p>上面我们已经开发了一个调度器，我们只要将对应的二进制文件制作成镜像，然后运行在 k8s 上，并将这个调度器命一个名，比如 my-scheduler，给后来的 pod 进行指定。当然可以参考<a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/tasks/administer-cluster/configure-multiple-schedulers/">官方文档</a>。</p>
<p>在整个集群中还可以同时运行多个调度器实例，通过 podSpec.schedulerName 来选择使用哪一个调度器（默认使用内置的调度器）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  schedulerName: my-scheduler  # 选择使用自定义调度器 my-scheduler</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx:1.10</span><br></pre></td></tr></table></figure>

<h2 id="调度框架"><a href="#调度框架" class="headerlink" title="调度框架"></a>调度框架</h2><p>Scheduler Framework 将 Pod 的调度过程分为两步：调度（过滤打分）和绑定（延时绑定和绑定）。</p>
<p>调度是为 Pod 选择一个合适的节点，而绑定则是将调度结果提交给集群。调度是顺序执行的，绑定并发执行。无论是在调度还是绑定过程中，如果发生错误或者判断 Pod 不可调度，那么 Pod 就会被重新放回调度队列，等待重新调度。</p>
<p>调度框架定义了一组扩展点，用户可以实现扩展点定义的接口来定义自己的调度逻辑，并将扩展注册到扩展点上，调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。调度框架在预留扩展点时，都是有特定的目的，有些扩展点上的扩展可以改变调度程序的决策方法，有些扩展点上的扩展只是发送一个通知。</p>
<p>下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kingjcy.github.io/media/cloud/k8s/scheduler1"></p>
<p>做一个简单的说明</p>
<ul>
<li>QueueSort 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，QueueSort 扩展本质上只需要实现一个方法 Less(Pod1, Pod2) 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 QueueSort 插件生效。</li>
<li>Pre-filter 扩展用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件，如果 pre-filter 返回了 error，则调度过程终止。</li>
<li>Filter 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 filter 扩展；如果任何一个 filter 将节点标记为不可选，则余下的 filter 扩展将不会被执行。调度器可以同时对多个节点执行 filter 扩展。</li>
<li>Post-filter 是一个通知类型的扩展点，调用该扩展的参数是 filter 阶段结束后被筛选为可选节点的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息。</li>
<li>Scoring 扩展用于为所有可选节点进行打分，调度器将针对每一个节点调用 Soring 扩展，评分结果是一个范围内的整数。在 normalize scoring 阶段，调度器将会把每个 scoring 扩展对具体某个节点的评分结果和该扩展的权重合并起来，作为最终评分结果。</li>
<li>Normalize scoring 扩展在调度器对节点进行最终排序之前修改每个节点的评分结果，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 scoring 扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 normalize scoring 扩展一次。</li>
<li>Reserve 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况。（因为绑定 Pod 到节点上是异步发生的）。这是调度过程的最后一个步骤，Pod 进入 reserved 状态以后，要么在绑定失败时触发 Unreserve 扩展，要么在绑定成功时，由 Post-bind 扩展结束绑定过程。</li>
<li>Permit 扩展用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项：<ul>
<li>approve（批准）：当所有的 permit 扩展都 approve 了 Pod 与节点的绑定，调度器将继续执行绑定过程</li>
<li>deny（拒绝）：如果任何一个 permit 扩展 deny 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展</li>
<li>wait（等待）：如果一个 permit 扩展返回了 wait，则 Pod 将保持在 permit 阶段，直到被其他扩展 approve，如果超时事件发生，wait 状态变成 deny，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展</li>
</ul>
</li>
<li>Pre-bind 扩展用于在 Pod 绑定之前执行某些逻辑。例如，pre-bind 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 pre-bind 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展。</li>
<li>Bind 扩展用于将 Pod 绑定到节点上：<ul>
<li>只有所有的 pre-bind 扩展都成功执行了，bind 扩展才会执行</li>
<li>调度框架按照 bind 扩展注册的顺序逐个调用 bind 扩展</li>
<li>具体某个 bind 扩展可以选择处理或者不处理该 Pod</li>
<li>如果某个 bind 扩展处理了该 Pod 与节点的绑定，余下的 bind 扩展将被忽略</li>
</ul>
</li>
<li>Post-bind 是一个通知性质的扩展：<ul>
<li>Post-bind 扩展在 Pod 成功绑定到节点上之后被动调用</li>
<li>Post-bind 扩展是绑定过程的最后一个步骤，可以用来执行资源清理的动作</li>
</ul>
</li>
<li>Unreserve 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 unreserve 扩展将被调用。Unreserve 扩展应该释放已经为 Pod 预留的节点上的计算资源。在一个插件中，reserve 扩展和 unreserve 扩展应该成对出现。</li>
</ul>
<p>一个 Plugin 可以实现多个扩展点。即在一个 Plugin 中既可以实现 Filter，又可以实现 Scoring，也可以再实现 Pre-Bind，看具体需求和场景，避免了一个需求实现多个 Plugin 的情况。</p>
<p>上述这些可插拔式逻辑，都是标准的 Go 语言插件机制（Go plugin 机制），也就是说，你需要在编译的时候选择把哪些插件编译进去。</p>
<p>如果我们要实现自己的插件，必须向调度框架注册插件并完成配置，另外还必须实现扩展点接口，对应的扩展点接口我们可以在源码 pkg&#x2F;scheduler&#x2F;framework&#x2F;v1alpha1&#x2F;interface.go 文件中找到，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">// Plugin is the parent type for all the scheduling framework plugins.</span><br><span class="line">type Plugin interface &#123;</span><br><span class="line">    Name() string</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type QueueSortPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Less(*PodInfo, *PodInfo) bool</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// PreFilterPlugin is an interface that must be implemented by &quot;prefilter&quot; plugins.</span><br><span class="line">// These plugins are called at the beginning of the scheduling cycle.</span><br><span class="line">type PreFilterPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PreFilter(pc *PluginContext, p *v1.Pod) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// FilterPlugin is an interface for Filter plugins. These plugins are called at the</span><br><span class="line">// filter extension point for filtering out hosts that cannot run a pod.</span><br><span class="line">// This concept used to be called &#x27;predicate&#x27; in the original scheduler.</span><br><span class="line">// These plugins should return &quot;Success&quot;, &quot;Unschedulable&quot; or &quot;Error&quot; in Status.code.</span><br><span class="line">// However, the scheduler accepts other valid codes as well.</span><br><span class="line">// Anything other than &quot;Success&quot; will lead to exclusion of the given host from</span><br><span class="line">// running the pod.</span><br><span class="line">type FilterPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Filter(pc *PluginContext, pod *v1.Pod, nodeName string) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// PostFilterPlugin is an interface for Post-filter plugin. Post-filter is an</span><br><span class="line">// informational extension point. Plugins will be called with a list of nodes</span><br><span class="line">// that passed the filtering phase. A plugin may use this data to update internal</span><br><span class="line">// state or to generate logs/metrics.</span><br><span class="line">type PostFilterPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PostFilter(pc *PluginContext, pod *v1.Pod, nodes []*v1.Node, filteredNodesStatuses NodeToStatusMap) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// ScorePlugin is an interface that must be implemented by &quot;score&quot; plugins to rank</span><br><span class="line">// nodes that passed the filtering phase.</span><br><span class="line">type ScorePlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Score(pc *PluginContext, p *v1.Pod, nodeName string) (int, *Status)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// ScoreWithNormalizePlugin is an interface that must be implemented by &quot;score&quot;</span><br><span class="line">// plugins that also need to normalize the node scoring results produced by the same</span><br><span class="line">// plugin&#x27;s &quot;Score&quot; method.</span><br><span class="line">type ScoreWithNormalizePlugin interface &#123;</span><br><span class="line">    ScorePlugin</span><br><span class="line">    NormalizeScore(pc *PluginContext, p *v1.Pod, scores NodeScoreList) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// ReservePlugin is an interface for Reserve plugins. These plugins are called</span><br><span class="line">// at the reservation point. These are meant to update the state of the plugin.</span><br><span class="line">// This concept used to be called &#x27;assume&#x27; in the original scheduler.</span><br><span class="line">// These plugins should return only Success or Error in Status.code. However,</span><br><span class="line">// the scheduler accepts other valid codes as well. Anything other than Success</span><br><span class="line">// will lead to rejection of the pod.</span><br><span class="line">type ReservePlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Reserve(pc *PluginContext, p *v1.Pod, nodeName string) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// PreBindPlugin is an interface that must be implemented by &quot;prebind&quot; plugins.</span><br><span class="line">// These plugins are called before a pod being scheduled.</span><br><span class="line">type PreBindPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PreBind(pc *PluginContext, p *v1.Pod, nodeName string) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// PostBindPlugin is an interface that must be implemented by &quot;postbind&quot; plugins.</span><br><span class="line">// These plugins are called after a pod is successfully bound to a node.</span><br><span class="line">type PostBindPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PostBind(pc *PluginContext, p *v1.Pod, nodeName string)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// UnreservePlugin is an interface for Unreserve plugins. This is an informational</span><br><span class="line">// extension point. If a pod was reserved and then rejected in a later phase, then</span><br><span class="line">// un-reserve plugins will be notified. Un-reserve plugins should clean up state</span><br><span class="line">// associated with the reserved Pod.</span><br><span class="line">type UnreservePlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Unreserve(pc *PluginContext, p *v1.Pod, nodeName string)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// PermitPlugin is an interface that must be implemented by &quot;permit&quot; plugins.</span><br><span class="line">// These plugins are called before a pod is bound to a node.</span><br><span class="line">type PermitPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Permit(pc *PluginContext, p *v1.Pod, nodeName string) (*Status, time.Duration)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// BindPlugin is an interface that must be implemented by &quot;bind&quot; plugins. Bind</span><br><span class="line">// plugins are used to bind a pod to a Node.</span><br><span class="line">type BindPlugin interface &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Bind(pc *PluginContext, p *v1.Pod, nodeName string) *Status</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们要实现对应的插件只要实现上面对应的插件接口，也可以在一个进程中实现那多个插件。我们可以看到每个插件都组合了 Plugin 的结构体，这个就是默认的实现方式，也就是我们对应的原生的策略，最终使用什么还是要在策略文件中使用 enable 来完成指定插件名的调用。</p>
<p>对于调度框架插件的启用或者禁用，我们同样可以使用上面的 KubeSchedulerConfiguration 资源对象来进行配置。下面的例子中的配置启用了一个实现了 reserve 和 preBind 扩展点的插件，并且禁用了另外一个插件，同时为插件 foo 提供了一些配置信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">plugins:</span><br><span class="line">  reserve:</span><br><span class="line">    enabled:</span><br><span class="line">    - name: foo</span><br><span class="line">    - name: bar</span><br><span class="line">    disabled:</span><br><span class="line">    - name: baz</span><br><span class="line">  preBind:</span><br><span class="line">    enabled:</span><br><span class="line">    - name: foo</span><br><span class="line">    disabled:</span><br><span class="line">    - name: baz</span><br><span class="line"></span><br><span class="line">pluginConfig:</span><br><span class="line">- name: foo</span><br><span class="line">  args: &gt;</span><br><span class="line">    foo插件可以解析的任意内容</span><br></pre></td></tr></table></figure>

<p>扩展的调用顺序如下：</p>
<ul>
<li>如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展</li>
<li>如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展</li>
<li>默认插件的扩展始终被最先调用，然后按照 KubeSchedulerConfiguration 中扩展的激活 enabled 顺序逐个调用扩展点的扩展</li>
<li>可以先禁用默认插件的扩展，然后在 enabled 列表中的某个位置激活默认插件的扩展，这种做法可以改变默认插件的扩展被调用时的顺序</li>
</ul>
<p>假设默认插件 foo 实现了 reserve 扩展点，此时我们要添加一个插件 bar，想要在 foo 之前被调用，则应该先禁用 foo 再按照 bar foo 的顺序激活。示例配置如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">plugins:</span><br><span class="line">  reserve:</span><br><span class="line">    enabled:</span><br><span class="line">    - name: bar</span><br><span class="line">    - name: foo</span><br><span class="line">    disabled:</span><br><span class="line">    - name: foo</span><br></pre></td></tr></table></figure>

<p>在源码目录 pkg&#x2F;scheduler&#x2F;framework&#x2F;plugins&#x2F;examples 中有几个示范插件，我们可以参照其实现方式。</p>
<h3 id="简单实现一个调度扩展插件"><a href="#简单实现一个调度扩展插件" class="headerlink" title="简单实现一个调度扩展插件"></a>简单实现一个调度扩展插件</h3><blockquote>
<p>注册</p>
</blockquote>
<p>其实要实现一个调度框架的插件，并不难，我们只要实现对应的扩展点，然后将插件注册到调度器中即可，我们来看一下注册：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">    rand.Seed(time.Now().UTC().UnixNano())</span><br><span class="line"></span><br><span class="line">    command := app.NewSchedulerCommand(</span><br><span class="line">        app.WithPlugin(sample.Name, sample.New),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    logs.InitLogs()</span><br><span class="line">    defer logs.FlushLogs()</span><br><span class="line"></span><br><span class="line">    if err := command.Execute(); err != nil &#123;</span><br><span class="line">        _, _ = fmt.Fprintf(os.Stderr, &quot;%v\n&quot;, err)</span><br><span class="line">        os.Exit(1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中 app.WithPlugin(sample.Name, sample.New) 就是注册一个名为 sample.Name 的插件，接下来就是我们接下来要实现的插件，从 WithPlugin 函数的参数也可以看出我们这里的 sample.New 必须是一个 framework.PluginFactory 类型的值，而 PluginFactory 的定义就是一个函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)</span><br></pre></td></tr></table></figure>

<p>我们简单看一下 WithPlugin 的函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// WithPlugin creates an Option based on plugin name and factory.</span><br><span class="line">func WithPlugin(name string, factory framework.PluginFactory) Option &#123;</span><br><span class="line">    return func(registry framework.Registry) error &#123;</span><br><span class="line">        return registry.Register(name, factory)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>创建的就是 Option 的列表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// Option configures a framework.Registry.</span><br><span class="line">type Option func(framework.Registry) error</span><br></pre></td></tr></table></figure>

<p>然后给 NewSchedulerCommand 调用，也就是 main 函数中的调用的。</p>
<blockquote>
<p>实现</p>
</blockquote>
<p>所以 sample.New 实际上就是上面的这个函数，在这个函数中我们可以获取到插件中的一些数据然后进行逻辑处理即可，插件实现如下所示，我们这里只是简单获取下数据打印日志，如果你有实际需求的可以根据获取的数据就行处理即可，我们这里只是实现了 PreFilter、Filter、PreBind 三个扩展点，其他的可以用同样的方式来扩展即可：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">// 插件名称</span><br><span class="line">const Name = &quot;sample-plugin&quot;</span><br><span class="line"></span><br><span class="line">type Args struct &#123;</span><br><span class="line">    FavoriteColor  string `json:&quot;favorite_color,omitempty&quot;`</span><br><span class="line">    FavoriteNumber int    `json:&quot;favorite_number,omitempty&quot;`</span><br><span class="line">    ThanksTo       string `json:&quot;thanks_to,omitempty&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Sample struct &#123;</span><br><span class="line">    args   *Args</span><br><span class="line">    handle framework.FrameworkHandle</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (s *Sample) Name() string &#123;</span><br><span class="line">    return Name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (s *Sample) PreFilter(pc *framework.PluginContext, pod *v1.Pod) *framework.Status &#123;</span><br><span class="line">    klog.V(3).Infof(&quot;prefilter pod: %v&quot;, pod.Name)</span><br><span class="line">    return framework.NewStatus(framework.Success, &quot;&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (s *Sample) Filter(pc *framework.PluginContext, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line">    klog.V(3).Infof(&quot;filter pod: %v, node: %v&quot;, pod.Name, nodeName)</span><br><span class="line">    return framework.NewStatus(framework.Success, &quot;&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (s *Sample) PreBind(pc *framework.PluginContext, pod *v1.Pod, nodeName string) *framework.Status &#123;</span><br><span class="line">    if nodeInfo, ok := s.handle.NodeInfoSnapshot().NodeInfoMap[nodeName]; !ok &#123;</span><br><span class="line">        return framework.NewStatus(framework.Error, fmt.Sprintf(&quot;prebind get node info error: %+v&quot;, nodeName))</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        klog.V(3).Infof(&quot;prebind node info: %+v&quot;, nodeInfo.Node())</span><br><span class="line">        return framework.NewStatus(framework.Success, &quot;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)</span><br><span class="line">func New(configuration *runtime.Unknown, f framework.FrameworkHandle) (framework.Plugin, error) &#123;</span><br><span class="line">    args := &amp;Args&#123;&#125;</span><br><span class="line">    if err := framework.DecodeInto(configuration, args); err != nil &#123;</span><br><span class="line">        return nil, err</span><br><span class="line">    &#125;</span><br><span class="line">    klog.V(3).Infof(&quot;get plugin config args: %+v&quot;, args)</span><br><span class="line">    return &amp;Sample&#123;</span><br><span class="line">        args: args,</span><br><span class="line">        handle: f,</span><br><span class="line">    &#125;, nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>完整代码可以前往仓库 <a target="_blank" rel="noopener" href="https://github.com/cnych/sample-scheduler-framework">https://github.com/cnych/sample-scheduler-framework</a> 获取。</p>
<blockquote>
<p>打包运行</p>
</blockquote>
<p>实现完成后，编译打包成镜像即可，然后我们就可以当成普通的应用用一个 Deployment 控制器来部署即可，由于我们需要去获取集群中的一些资源对象，所以当然需要申请 RBAC 权限，然后同样通过 –config 参数来配置我们的调度器，同样还是使用一个 KubeSchedulerConfiguration 资源对象配置，可以通过 plugins 来启用或者禁用我们实现的插件，也可以通过 pluginConfig 来传递一些参数值给插件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: sample-scheduler-clusterrole</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - pods</span><br><span class="line">    verbs:</span><br><span class="line">      - delete</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - bindings</span><br><span class="line">      - pods/binding</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - pods/status</span><br><span class="line">    verbs:</span><br><span class="line">      - patch</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - replicationcontrollers</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - apps</span><br><span class="line">      - extensions</span><br><span class="line">    resources:</span><br><span class="line">      - replicasets</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - apps</span><br><span class="line">    resources:</span><br><span class="line">      - statefulsets</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - policy</span><br><span class="line">    resources:</span><br><span class="line">      - poddisruptionbudgets</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - persistentvolumeclaims</span><br><span class="line">      - persistentvolumes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;storage.k8s.io&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - storageclasses</span><br><span class="line">      - csinodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;coordination.k8s.io&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - leases</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;events.k8s.io&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - patch</span><br><span class="line">      - update</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: sample-scheduler-sa</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: sample-scheduler-clusterrolebinding</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: sample-scheduler-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: sample-scheduler-sa</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: scheduler-config</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  scheduler-config.yaml: |</span><br><span class="line">    apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">    kind: KubeSchedulerConfiguration</span><br><span class="line">    schedulerName: sample-scheduler</span><br><span class="line">    leaderElection:</span><br><span class="line">      leaderElect: true</span><br><span class="line">      lockObjectName: sample-scheduler</span><br><span class="line">      lockObjectNamespace: kube-system</span><br><span class="line">    plugins:</span><br><span class="line">      preFilter:</span><br><span class="line">        enabled:</span><br><span class="line">        - name: &quot;sample-plugin&quot;</span><br><span class="line">      filter:</span><br><span class="line">        enabled:</span><br><span class="line">        - name: &quot;sample-plugin&quot;</span><br><span class="line">      preBind:</span><br><span class="line">        enabled:</span><br><span class="line">        - name: &quot;sample-plugin&quot;</span><br><span class="line">    pluginConfig:</span><br><span class="line">    - name: &quot;sample-plugin&quot;</span><br><span class="line">      args:</span><br><span class="line">        favorite_color: &quot;#326CE5&quot;</span><br><span class="line">        favorite_number: 7</span><br><span class="line">        thanks_to: &quot;thockin&quot;</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: sample-scheduler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    component: sample-scheduler</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      component: sample-scheduler</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        component: sample-scheduler</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccount: sample-scheduler-sa</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      volumes:</span><br><span class="line">        - name: scheduler-config</span><br><span class="line">          configMap:</span><br><span class="line">            name: scheduler-config</span><br><span class="line">      containers:</span><br><span class="line">        - name: scheduler-ctrl</span><br><span class="line">          image: cnych/sample-scheduler:v0.1.6</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          args:</span><br><span class="line">            - sample-scheduler-framework</span><br><span class="line">            - --config=/etc/kubernetes/scheduler-config.yaml</span><br><span class="line">            - --v=3</span><br><span class="line">          resources:</span><br><span class="line">            requests:</span><br><span class="line">              cpu: &quot;50m&quot;</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: scheduler-config</span><br><span class="line">              mountPath: /etc/kubernetes</span><br></pre></td></tr></table></figure>

<p>直接部署上面的资源对象即可，这样我们就部署了一个名为 sample-scheduler 的调度器了。</p>
<blockquote>
<p>测试</p>
</blockquote>
<p>接下来我们可以部署一个应用来使用这个调度器进行调度：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: test-scheduler</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: test-scheduler</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: test-scheduler</span><br><span class="line">    spec:</span><br><span class="line">      schedulerName: sample-scheduler</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>

<p>这里需要注意的是我们现在手动指定了一个 schedulerName 的字段，将其设置成上面我们自定义的调度器名称 sample-scheduler。</p>
<p>我们直接创建这个资源对象，创建完成后查看我们自定义调度器的日志信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system -l component=sample-scheduler</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">sample-scheduler-7c469787f-rwhhd   1/1     Running   0          13m</span><br><span class="line">$ kubectl logs -f sample-scheduler-7c469787f-rwhhd -n kube-system</span><br><span class="line">I0104 08:24:22.087881       1 scheduler.go:530] Attempting to schedule pod: default/test-scheduler-6d779d9465-rq2bb</span><br><span class="line">I0104 08:24:22.087992       1 plugins.go:23] prefilter pod: test-scheduler-6d779d9465-rq2bb</span><br><span class="line">I0104 08:24:22.088657       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node1</span><br><span class="line">I0104 08:24:22.088797       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node2</span><br><span class="line">I0104 08:24:22.088871       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node3</span><br><span class="line">I0104 08:24:22.088946       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-node4</span><br><span class="line">I0104 08:24:22.088992       1 plugins.go:28] filter pod: test-scheduler-6d779d9465-rq2bb, node: ydzs-master</span><br><span class="line">I0104 08:24:22.090653       1 plugins.go:36] prebind node info: &amp;Node&#123;ObjectMeta:&#123;ydzs-node3   /api/v1/nodes/ydzs-node3 1ff6e228-4d98-4737-b6d3-30a5d55ccdc2 15466372 0 2019-11-10 09:05:09 +0000 UTC &lt;nil&gt; &lt;nil&gt; ......&#125;</span><br><span class="line">I0104 08:24:22.091761       1 factory.go:610] Attempting to bind test-scheduler-6d779d9465-rq2bb to ydzs-node3</span><br><span class="line">I0104 08:24:22.104994       1 scheduler.go:667] pod default/test-scheduler-6d779d9465-rq2bb is bound successfully on node &quot;ydzs-node3&quot;, 5 nodes evaluated, 4 nodes were found feasible. Bound node resource: &quot;Capacity: CPU&lt;4&gt;|Memory&lt;8008820Ki&gt;|Pods&lt;110&gt;|StorageEphemeral&lt;17921Mi&gt;; Allocatable: CPU&lt;4&gt;|Memory&lt;7906420Ki&gt;|Pods&lt;110&gt;|StorageEphemeral&lt;16912377419&gt;.&quot;.</span><br><span class="line">可以看到当我们创建完 Pod 后，在我们自定义的调度器中就出现了对应的日志，并且在我们定义的扩展点上面都出现了对应的日志，证明我们的示例成功了，也可以通过查看 Pod 的 schedulerName 来验证：</span><br><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">test-scheduler-6d779d9465-rq2bb           1/1     Running   0          22m</span><br><span class="line">$ kubectl get pod test-scheduler-6d779d9465-rq2bb -o yaml</span><br><span class="line">......</span><br><span class="line">restartPolicy: Always</span><br><span class="line">schedulerName: sample-scheduler</span><br><span class="line">securityContext: &#123;&#125;</span><br><span class="line">serviceAccount: default</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>在最新的 Kubernetes v1.17 版本中，Scheduler Framework 内置的预选和优选函数已经全部插件化，所以要扩展调度器我们应该掌握并理解调度框架这种方式。所以以后调度框架是必然的趋势，我们所要做的工作就是将我们开发的调度器或扩展程序移植到我们对应的调度框架中去。</p>
<p>kubernetes 调度器的源码位于 kubernetes&#x2F;pkg&#x2F;scheduler 中，大体的代码目录结构如下所示：(不同的版本目录结构可能不太一样)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubernetes/pkg/scheduler</span><br><span class="line">-- scheduler.go         //调度相关的具体实现</span><br><span class="line">|-- algorithm</span><br><span class="line">|   |-- predicates      //节点筛选策略</span><br><span class="line">|   |-- priorities      //节点打分策略</span><br><span class="line">|-- algorithmprovider</span><br><span class="line">|   |-- defaults         //定义默认的调度器</span><br></pre></td></tr></table></figure>

<p>其中 Scheduler 创建和运行的核心程序，对应的代码在 pkg&#x2F;scheduler&#x2F;scheduler.go，如果要查看 kube-scheduler 的入口程序，对应的代码在 cmd&#x2F;kube-scheduler&#x2F;scheduler.go。主要调度就是上面的流程。</p>
<h2 id="批调度"><a href="#批调度" class="headerlink" title="批调度"></a>批调度</h2><p>Coscheduling 的定义是“在并发系统中将多个相关联的进程调度到不同处理器上同时运行的策略”。在 Coscheduling 的场景中，最主要的原则是保证所有相关联的进程能够同时启动。防止部分进程的异常，导致整个关联进程组的阻塞。这种导致阻塞的部分异常进程，称之为“碎片（fragement）”。 在 Coscheduling 的具体实现过程中，根据是否允许 “碎片” 存在，可以细分为 Explicit Coscheduling，Local Coscheduling 和 Implicit Coscheduling。 其中 Explicit Coscheduling 就是大家常听到的 Gang Scheduling。Gang Scheduling 要求完全不允许有 “碎片” 存在， 也就是“All or Nothing”。</p>
<p>简单来说，一个批任务（关联进程组）包括了 N 个 Pod（进程），Kubernetes 调度器负责将这 N 个 Pod 调度到 M 个节点（处理器）上同时运行。如果这个批任务需要部分 Pod 同时启动即可运行，我们称需启动 Pod 的最小数量为 min-available。特别地，当 min-available&#x3D;N 时，批任务要求满足 Gang Scheduling。</p>
<p>为什么需要批调度？</p>
<p>JobA 需要 4 个 Pod 同时启动，才能正常运行。Kube-scheduler 依次调度 3 个 Pod 并创建成功。到第 4 个 Pod 时，集群资源不足，则 JobA 的 3 个 Pod 处于空等的状态。但是它们已经占用了部分资源，如果第 4 个 Pod 不能及时启动的话，整个 JobA 无法成功运行，更糟糕的是导致集群资源浪费。如果出现更坏的情况的话，如下图所示，集群其他的资源刚好被 JobB 的 3 个 Pod 所占用，同时在等待 JobB 的第 4 个 Pod 创建，此时整个集群就出现了死锁。</p>
<p>解决</p>
<p>社区目前有 Kube-batch 以及基于 Kube-batch 衍生的 Volcano 2 个项目来解决上文中提到的痛点。实现的方式是通过开发新的调度器将 Scheduler 中的调度单元从 Pod 修改为 PodGroup，以组的形式进行调度。使用方式是如果需要 Coscheduling 功能的 Pod 走新的调度器</p>
<p>这些方案虽然能够解决 Coscheduling 的问题，但是同样引入了新的问题。如大家所知，对于同一集群资源，调度器需要中心化。但如果同时存在两个调度器的话，有可能会出现决策冲突，例如分别将同一块资源分配给两个不同的 Pod，导致某个 Pod 调度到节点后因为资源不足，导致无法创建的问题。解决的方式只能是通过标签的形式将节点强行的划分开来，或者部署多个集群。这种方式通过同一个 Kubernetes 集群来同时运行在线服务和离线作业，势必会导致整体集群资源的浪费以及运维成本的增加。再者，Volcano 运行需要启动定制的 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。这些 Webhooks 本身存在单点风险，一旦出现故障，将影响集群内所有 pod 的创建。另外，多运行一套调度器，本身也会带来维护上的复杂性，以及与上游 Kube-scheduler 接口兼容上的不确定性。</p>
<p>基于 Scheduling Framework 的方案</p>
<p>实现</p>
<p>PodGroup</p>
<p>我们通过 label 的形式来定义 PodGroup 的概念，拥有同样 label 的 Pod 同属于一个 PodGroup。min-available 是用来标识该 PodGroup 的作业能够正式运行时所需要的最小副本数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">labels:</span><br><span class="line">     pod-group.scheduling.sigs.k8s.io/name: tf-smoke-gpu</span><br><span class="line">     pod-group.scheduling.sigs.k8s.io/min-available: &quot;2&quot;</span><br><span class="line">备注: 要求属于同一个PodGroup的Pod必须保持相同的优先级</span><br></pre></td></tr></table></figure>

<p>Permit</p>
<p>Framework 的 Permit 插件提供了延迟绑定的功能，即 Pod 进入到 Permit 阶段时，用户可以自定义条件来允许 Pod 通过、拒绝 Pod 通过以及让 Pod 等待状态 (可设置超时时间)。Permit 的延迟绑定的功能，刚好可以让属于同一个 PodGruop 的 Pod 调度到这个节点时，进行等待，等待积累的 Pod 数目满足足够的数目时，再统一运行同一个 PodGruop 的所有 Pod 进行绑定并创建。</p>
<p>举个实际的例子，当 JobA 调度时，需要 4 个 Pod 同时启动，才能正常运行。但此时集群仅能满足 3 个 Pod 创建，此时与 Default Scheduler 不同的是，并不是直接将 3 个 Pod 调度并创建。而是通过 Framework 的 Permit 机制进行等待。</p>
<p>此时当集群中有空闲资源被释放后，JobA 的中 Pod 所需要的资源均可以满足。</p>
<p>则 JobA 的 4 个 Pod 被一起调度创建出来，正常运行任务。</p>
<p>QueueSort</p>
<p>由于 Default Scheduler 的队列并不能感知 PodGroup 的信息，所以 Pod 在出队时处于无序性 (针对 PodGroup 而言)。如下图所示，a 和 b 表示两个不同的 PodGroup，两个 PodGroup 的 Pod 在进入队列时，由于创建的时间交错导致在队列中以交错的顺序排列。</p>
<p>当一个新的 Pod 创建后，入队后，无法跟与其相同的 PodGroup 的 Pod 排列在一起，只能继续以混乱的形式交错排列。</p>
<p>这种无序性就会导致如果 PodGroupA 在 Permit 阶段处于等待状态，此时 PodGroupB 的 Pod 调度完成后也处于等待状态，相互占有资源使得 PodGroupA 和 PodGroupB 均无法正常调度。这种情况即是把死锁现象出现的位置从 Node 节点移动到 Permit 阶段，无法解决前文提到的问题。</p>
<p>针对如上所示的问题，我们通过实现 QueueSort 插件， 保证在队列中属于同一个 PodGroup 的 Pod 能够排列在一起。我们通过定义 QueueSort 所用的 Less 方法，作用于 Pod 在入队后排队的顺序：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">func  Less(podA *PodInfo, podB *PodInfo) bool</span><br></pre></td></tr></table></figure>

<p>首先，继承了默认的基于优先级的比较方式，高优先级的 Pod 会排在低优先级的 Pod 之前。 然后，如果两个 Pod 的优先级相同，我们定义了新的排队逻辑来支持 PodGroup 的排序。</p>
<p>如果两个 Pod 都是 regularPod(普通的 Pod)，则谁先创建谁在队列里边排在前边。 如果两个 Pod 一个是 regularPod，另一个是 pgPod(属于某个 PodGroup 的 Pod), 我们比较的是 regularPod 的创建时间和 pgPod 所属 PodGroup 的创建时间，则谁先创建谁在队列里边排在前边。 如果两个 Pod 都是 pgPod，我们比较两个 PodGroup 的创建时间，则谁先创建谁在队列里边排在前边。同时有可能两个 PodGroup 的创建时间相同，我们引入了自增 Id，使得 PodGroup 的 Id 谁小谁排在前边 (此处的目的是为了区分不同的 PodGroup)。 通过如上的排队策略，我们实现属于同一个 PodGroup 的 Pod 能够同一个 PodGroup 的 Pod 能够排列在一起。</p>
<p>当一个新的 Pod 创建后，入队后，会跟与其相同的 PodGroup 的 Pod 排列在一起。</p>
<p>Prefilter</p>
<p>为了减少无效的调度操作，提升调度的性能，我们在 Prefilter 阶段增加一个过滤条件，当一个 Pod 调度时，会计算该 Pod 所属 PodGroup 的 Pod 的 Sum(包括 Running 状态的)，如果 Sum 小于 min-available 时，则肯定无法满足 min-available 的要求，则直接在 Prefilter 阶段拒绝掉，不再进入调度的主流程。</p>
<p>UnReserve</p>
<p>如果某个 Pod 在 Permit 阶段等待超时了，则会进入到 UnReserve 阶段，我们会直接拒绝掉所有跟 Pod 属于同一个 PodGroup 的 Pod，避免剩余的 Pod 进行长时间的无效等待。</p>
<h2 id="Binpack"><a href="#Binpack" class="headerlink" title="Binpack"></a>Binpack</h2><p>为什么需要 Binpack 功能？</p>
<p>Kubernetes 默认开启的资源调度策略是 LeastRequestedPriority，消耗的资源最少的节点会优先被调度，使得整体集群的资源使用在所有节点之间分配地相对均匀。但是这种调度策略往往也会在单个节点上产生较多资源碎片。 比如两个节点各剩余 1GPU 的资源。这是有申请 2GPU 的新作业，提交到调度器，则因为无法提供足够的资源，导致调度失败。如上这种情况情况，每个节点都有 1 个 GPU 卡空闲，可是又无法被利用，导致资源 GPU 这种昂贵的资源被浪费。如果使用的资源调度策略是 Binpack，优先将节点资源填满之后，再调度下一个节点，则上图所出现的资源碎片问题得到解决。申请 2GPU 的作业被正常调度到节点上，提升了集群的资源使用率。</p>
<p>Binpack 实现已经抽象成 Kubernetes Scheduler Framework 的 Score 插件 RequestedToCapacityRatio（根据已分配资源的某函数设置选择节点。 实现的扩展点： Score 。），用于优选阶段给节点打分。将节点根据自己定义的配置进行打分。具体的实现可以分为两个部分，构建打分函数和打分.</p>
<ul>
<li><p>构建打分函数的过程比较容易理解，就是用户可以自己定义不同的利用率所对应的分值大小，以便影响调度的决策过程。即如果资源利用率为 0 的时候，得分为 0 分，当资源利用率为 100 时，得分为 10 分，所以得到的资源利用率越高，得分越高，则这个行为是 Binpack 的资源分配方式。用户也可以设置成利用率为 0 时，得分为 10 分，利用率为 100 时，得分为 0 分。这样意味着资源利用率越低，则得分越高，这种行为是 spreading 的资源分配方式。</p>
</li>
<li><p>打分用户可以自己定义在 Binpack 计算中所要参考的资源以及权重值，例如可以只是设定 GPU 和 CPU 的值和权重。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">resourcetoweightmap:</span><br><span class="line">    &quot;cpu&quot;: 1</span><br><span class="line">    &quot;nvidia.com/gpu&quot;: 1</span><br></pre></td></tr></table></figure></li>
</ul>
<p>然后在打分过程总，会通过计算 (pod.Request + node.Allocated)&#x2F;node.Total 的结果得到对应资源的利用率，并且将利用率带入上文中所述的打分函数中，得到相应的分数。最后将所有的资源根据 weight 值，加权得到最终的分数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Score = line(resource1_utilization) * weight1 + line(resource2_utilization) * weight2 ....) /</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">哪吒藕霸</div><div class="post-copyright__author_desc"></div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://shippomx.github.io/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://shippomx.github.io/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/')">Kubernetes scheduler详解</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://shippomx.github.io/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=Kubernetes scheduler详解&amp;url=https://shippomx.github.io/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="rm.copyPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://shippomx.github.io" target="_blank">远辰</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/container/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>container<span class="tagsPageCount">27</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/04/containers/Calico%20%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E6%8F%AD%E7%A7%98/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Calico网络通信简略原理</div></div></a></div><div class="next-post pull-right"><a href="/2023/10/06/containers/Podman%E7%9A%84%E4%B8%80%E4%BA%9B%E7%89%B9%E6%80%A7/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Podman 的特性概述</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2023/10/04/containers/Calico%20%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E6%8F%AD%E7%A7%98/" title="Calico网络通信简略原理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-10-04</div><div class="title">Calico网络通信简略原理</div></div></a></div><div><a href="/2021/12/21/containers/Docker%20%E7%9A%84%E7%BD%91%E7%BB%9C%20%E5%B0%86%E5%AE%B9%E5%99%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E4%B8%96%E7%95%8C%E8%BF%9E%E6%8E%A5/" title="docker中的网络"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-12-21</div><div class="title">docker中的网络</div></div></a></div><div><a href="/2021/12/21/containers/Docker%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%AE%B9%E5%99%A8%E9%97%B4%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/" title="docker中的网络模式"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-12-21</div><div class="title">docker中的网络模式</div></div></a></div><div><a href="/2023/10/07/containers/Docker%E5%AE%B9%E5%99%A8%EF%BC%9A%E8%99%9A%E6%8B%9F%E5%8C%96/" title="Docker与虚拟化"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-10-07</div><div class="title">Docker与虚拟化</div></div></a></div><div><a href="/2023/10/06/containers/Kubernetes%20CNI%20%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6/" title="Kubernetes CNI 网络插件"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-10-06</div><div class="title">Kubernetes CNI 网络插件</div></div></a></div><div><a href="/2020/12/20/containers/Kata%20Containers%202.0%20%E4%BB%8B%E7%BB%8D/" title="Kata Containers 2.0"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2020-12-20</div><div class="title">Kata Containers 2.0</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description"></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%A9%E7%90%86%E9%83%A8%E7%BD%B2"><span class="toc-number">1.</span> <span class="toc-text">物理部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">2.</span> <span class="toc-text">配置文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%B9%E5%99%A8%E9%83%A8%E7%BD%B2"><span class="toc-number">3.</span> <span class="toc-text">容器部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text">高可用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E9%80%89%E7%AD%96%E7%95%A5"><span class="toc-number">5.</span> <span class="toc-text">预选策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GeneralPredicates"><span class="toc-number">5.1.</span> <span class="toc-text">GeneralPredicates</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Volume-%E7%9B%B8%E5%85%B3"><span class="toc-number">5.2.</span> <span class="toc-text">Volume 相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%BF%E4%B8%BB%E6%9C%BA%E7%9B%B8%E5%85%B3"><span class="toc-number">5.3.</span> <span class="toc-text">宿主机相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">5.4.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E9%80%89%E7%AD%96%E7%95%A5"><span class="toc-number">6.</span> <span class="toc-text">优选策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#leastRequestedPriority"><span class="toc-number">6.1.</span> <span class="toc-text">leastRequestedPriority</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CalculateNodeLabelPriority"><span class="toc-number">6.2.</span> <span class="toc-text">CalculateNodeLabelPriority</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BalancedResourceAllocation"><span class="toc-number">6.3.</span> <span class="toc-text">BalancedResourceAllocation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96-1"><span class="toc-number">6.4.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%B2%E5%92%8C%E6%80%A7"><span class="toc-number">7.</span> <span class="toc-text">亲和性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A-Node-%E8%8A%82%E7%82%B9%E8%B0%83%E5%BA%A6"><span class="toc-number">7.1.</span> <span class="toc-text">指定 Node 节点调度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%B2%E5%92%8C%E6%80%A7%EF%BC%88Affinity%EF%BC%89%E4%B8%8E%E9%9D%9E%E4%BA%B2%E5%92%8C%E6%80%A7%EF%BC%88anti-affinity%EF%BC%89"><span class="toc-number">7.2.</span> <span class="toc-text">亲和性（Affinity）与非亲和性（anti-affinity）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%8F%E8%94%BD-Node-%E8%8A%82%E7%82%B9%E8%B0%83%E5%BA%A6"><span class="toc-number">7.3.</span> <span class="toc-text">屏蔽 Node 节点调度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%85%88%E7%BA%A7%E8%B0%83%E5%BA%A6"><span class="toc-number">8.</span> <span class="toc-text">优先级调度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A2%E5%8D%A0%E5%BC%8F%E8%B0%83%E5%BA%A6"><span class="toc-number">8.1.</span> <span class="toc-text">抢占式调度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E5%99%A8%E6%89%A9%E5%B1%95%E7%A8%8B%E5%BA%8F"><span class="toc-number">9.</span> <span class="toc-text">调度器扩展程序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">10.</span> <span class="toc-text">多调度器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6"><span class="toc-number">11.</span> <span class="toc-text">调度框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%B0%83%E5%BA%A6%E6%89%A9%E5%B1%95%E6%8F%92%E4%BB%B6"><span class="toc-number">11.1.</span> <span class="toc-text">简单实现一个调度扩展插件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E8%B0%83%E5%BA%A6"><span class="toc-number">12.</span> <span class="toc-text">批调度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Binpack"><span class="toc-number">13.</span> <span class="toc-text">Binpack</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/08/cmake%20starter/" title="cmake starters">cmake starters</a><time datetime="2023-10-08T03:25:56.000Z" title="发表于 2023-10-08 11:25:56">2023-10-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/07/containers/Docker%E5%AE%B9%E5%99%A8%EF%BC%9A%E8%99%9A%E6%8B%9F%E5%8C%96/" title="Docker与虚拟化">Docker与虚拟化</a><time datetime="2023-10-07T06:44:56.000Z" title="发表于 2023-10-07 14:44:56">2023-10-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/06/containers/Kubernetes%20CNI%20%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6/" title="Kubernetes CNI 网络插件">Kubernetes CNI 网络插件</a><time datetime="2023-10-06T06:44:56.000Z" title="发表于 2023-10-06 14:44:56">2023-10-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/06/containers/Podman%E7%9A%84%E4%B8%80%E4%BA%9B%E7%89%B9%E6%80%A7/" title="Podman 的特性概述">Podman 的特性概述</a><time datetime="2023-10-06T06:44:56.000Z" title="发表于 2023-10-06 14:44:56">2023-10-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/06/containers/K8s%20scheduler%20%E8%AF%A6%E8%A7%A3/" title="Kubernetes scheduler详解">Kubernetes scheduler详解</a><time datetime="2023-10-06T06:44:56.000Z" title="发表于 2023-10-06 14:44:56">2023-10-06</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2023 By <a class="footer-bar-link" href="/" title="哪吒藕霸" target="_blank">哪吒藕霸</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">0</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/algorithm/" style="font-size: 0.88rem;">algorithm<sup>1</sup></a><a href="/tags/blockchain/" style="font-size: 0.88rem;">blockchain<sup>1</sup></a><a href="/tags/c/" style="font-size: 0.88rem;">c<sup>1</sup></a><a href="/tags/container/" style="font-size: 0.88rem;">container<sup>27</sup></a><a href="/tags/go/" style="font-size: 0.88rem;">go<sup>10</sup></a><a href="/tags/kidgets/" style="font-size: 0.88rem;">kidgets<sup>3</sup></a><a href="/tags/linux/" style="font-size: 0.88rem;">linux<sup>22</sup></a><a href="/tags/rust/" style="font-size: 0.88rem;">rust<sup>6</sup></a><a href="/tags/tools/" style="font-size: 0.88rem;">tools<sup>1</sup></a><a href="/tags/tvbox/" style="font-size: 0.88rem;">tvbox<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.4/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.8",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 哪吒藕霸 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "visitor@anheyu.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>